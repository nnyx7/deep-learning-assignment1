{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://deep-learning-su.github.io/assignment-requirements/) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from deep_learning_su.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'deep_learning_su/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **deep_learning_su/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.326474\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file deep_learning_su/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from deep_learning_su.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** I think it is because when our model is not trained, the possibility of each class is almost equal - in our case that possibility is 1/10 = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.250831 analytic: 1.250830, relative error: 3.642509e-08\n",
      "numerical: 1.112393 analytic: 1.112393, relative error: 1.047594e-08\n",
      "numerical: -1.459371 analytic: -1.459371, relative error: 4.335828e-08\n",
      "numerical: -6.262799 analytic: -6.262799, relative error: 5.801558e-09\n",
      "numerical: -0.058668 analytic: -0.058668, relative error: 1.235049e-06\n",
      "numerical: 0.152755 analytic: 0.152755, relative error: 1.756642e-07\n",
      "numerical: 0.103206 analytic: 0.103206, relative error: 1.057516e-07\n",
      "numerical: -0.068235 analytic: -0.068236, relative error: 5.835960e-07\n",
      "numerical: -0.275819 analytic: -0.275819, relative error: 1.891874e-08\n",
      "numerical: -0.484552 analytic: -0.484552, relative error: 1.813127e-08\n",
      "numerical: -0.599964 analytic: -0.599965, relative error: 1.212998e-07\n",
      "numerical: -0.625917 analytic: -0.625917, relative error: 8.651558e-08\n",
      "numerical: -1.874768 analytic: -1.874768, relative error: 1.813716e-08\n",
      "numerical: 3.218269 analytic: 3.218269, relative error: 1.526035e-08\n",
      "numerical: 0.618737 analytic: 0.618737, relative error: 1.659561e-07\n",
      "numerical: -0.678728 analytic: -0.678728, relative error: 4.966430e-08\n",
      "numerical: 0.708093 analytic: 0.708093, relative error: 3.168974e-08\n",
      "numerical: 0.717909 analytic: 0.717909, relative error: 2.767171e-08\n",
      "numerical: 0.361396 analytic: 0.361396, relative error: 2.319522e-07\n",
      "numerical: -0.188554 analytic: -0.188554, relative error: 9.072077e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from deep_learning_su.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.326474e+00 computed in 0.379555s\n",
      "vectorized loss: 2.326474e+00 computed in 0.014135s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from deep_learning_su.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 774.473019\n",
      "iteration 100 / 1500: loss 468.823508\n",
      "iteration 200 / 1500: loss 284.196468\n",
      "iteration 300 / 1500: loss 172.771420\n",
      "iteration 400 / 1500: loss 105.297166\n",
      "iteration 500 / 1500: loss 64.491071\n",
      "iteration 600 / 1500: loss 39.936079\n",
      "iteration 700 / 1500: loss 24.987558\n",
      "iteration 800 / 1500: loss 15.963899\n",
      "iteration 900 / 1500: loss 10.491749\n",
      "iteration 1000 / 1500: loss 7.160453\n",
      "iteration 1100 / 1500: loss 5.219558\n",
      "iteration 1200 / 1500: loss 3.958625\n",
      "iteration 1300 / 1500: loss 3.238314\n",
      "iteration 1400 / 1500: loss 2.775982\n",
      "iteration 0 / 1500: loss 859.845943\n",
      "iteration 100 / 1500: loss 491.694717\n",
      "iteration 200 / 1500: loss 281.992824\n",
      "iteration 300 / 1500: loss 162.112968\n",
      "iteration 400 / 1500: loss 93.631512\n",
      "iteration 500 / 1500: loss 54.487946\n",
      "iteration 600 / 1500: loss 32.019578\n",
      "iteration 700 / 1500: loss 19.261054\n",
      "iteration 800 / 1500: loss 11.981399\n",
      "iteration 900 / 1500: loss 7.681780\n",
      "iteration 1000 / 1500: loss 5.360393\n",
      "iteration 1100 / 1500: loss 3.938939\n",
      "iteration 1200 / 1500: loss 3.180135\n",
      "iteration 1300 / 1500: loss 2.709093\n",
      "iteration 1400 / 1500: loss 2.539628\n",
      "iteration 0 / 1500: loss 942.395740\n",
      "iteration 100 / 1500: loss 509.587717\n",
      "iteration 200 / 1500: loss 276.630700\n",
      "iteration 300 / 1500: loss 150.704651\n",
      "iteration 400 / 1500: loss 82.488049\n",
      "iteration 500 / 1500: loss 45.569926\n",
      "iteration 600 / 1500: loss 25.740305\n",
      "iteration 700 / 1500: loss 14.913561\n",
      "iteration 800 / 1500: loss 9.072840\n",
      "iteration 900 / 1500: loss 5.939791\n",
      "iteration 1000 / 1500: loss 4.124758\n",
      "iteration 1100 / 1500: loss 3.221518\n",
      "iteration 1200 / 1500: loss 2.738438\n",
      "iteration 1300 / 1500: loss 2.527453\n",
      "iteration 1400 / 1500: loss 2.310216\n",
      "iteration 0 / 1500: loss 1029.563185\n",
      "iteration 100 / 1500: loss 527.469022\n",
      "iteration 200 / 1500: loss 270.819710\n",
      "iteration 300 / 1500: loss 139.636258\n",
      "iteration 400 / 1500: loss 72.515808\n",
      "iteration 500 / 1500: loss 38.142066\n",
      "iteration 600 / 1500: loss 20.597948\n",
      "iteration 700 / 1500: loss 11.578706\n",
      "iteration 800 / 1500: loss 6.956318\n",
      "iteration 900 / 1500: loss 4.620904\n",
      "iteration 1000 / 1500: loss 3.409222\n",
      "iteration 1100 / 1500: loss 2.846680\n",
      "iteration 1200 / 1500: loss 2.457114\n",
      "iteration 1300 / 1500: loss 2.305847\n",
      "iteration 1400 / 1500: loss 2.279073\n",
      "iteration 0 / 1500: loss 1122.169337\n",
      "iteration 100 / 1500: loss 543.560261\n",
      "iteration 200 / 1500: loss 263.971999\n",
      "iteration 300 / 1500: loss 128.760114\n",
      "iteration 400 / 1500: loss 63.485343\n",
      "iteration 500 / 1500: loss 31.851275\n",
      "iteration 600 / 1500: loss 16.474273\n",
      "iteration 700 / 1500: loss 9.138434\n",
      "iteration 800 / 1500: loss 5.436629\n",
      "iteration 900 / 1500: loss 3.829272\n",
      "iteration 1000 / 1500: loss 2.877250\n",
      "iteration 1100 / 1500: loss 2.519959\n",
      "iteration 1200 / 1500: loss 2.351166\n",
      "iteration 1300 / 1500: loss 2.211100\n",
      "iteration 1400 / 1500: loss 2.135935\n",
      "iteration 0 / 1500: loss 1199.105249\n",
      "iteration 100 / 1500: loss 548.757164\n",
      "iteration 200 / 1500: loss 252.270349\n",
      "iteration 300 / 1500: loss 116.571727\n",
      "iteration 400 / 1500: loss 54.541162\n",
      "iteration 500 / 1500: loss 26.178043\n",
      "iteration 600 / 1500: loss 13.082639\n",
      "iteration 700 / 1500: loss 7.165239\n",
      "iteration 800 / 1500: loss 4.504997\n",
      "iteration 900 / 1500: loss 3.193841\n",
      "iteration 1000 / 1500: loss 2.633945\n",
      "iteration 1100 / 1500: loss 2.326281\n",
      "iteration 1200 / 1500: loss 2.182277\n",
      "iteration 1300 / 1500: loss 2.163603\n",
      "iteration 1400 / 1500: loss 2.125926\n",
      "iteration 0 / 1500: loss 1291.769663\n",
      "iteration 100 / 1500: loss 559.596604\n",
      "iteration 200 / 1500: loss 243.220174\n",
      "iteration 300 / 1500: loss 106.416586\n",
      "iteration 400 / 1500: loss 47.281957\n",
      "iteration 500 / 1500: loss 21.693382\n",
      "iteration 600 / 1500: loss 10.620985\n",
      "iteration 700 / 1500: loss 5.794137\n",
      "iteration 800 / 1500: loss 3.775118\n",
      "iteration 900 / 1500: loss 2.875325\n",
      "iteration 1000 / 1500: loss 2.489668\n",
      "iteration 1100 / 1500: loss 2.262778\n",
      "iteration 1200 / 1500: loss 2.179339\n",
      "iteration 1300 / 1500: loss 2.171541\n",
      "iteration 1400 / 1500: loss 2.210717\n",
      "iteration 0 / 1500: loss 1357.210054\n",
      "iteration 100 / 1500: loss 555.831412\n",
      "iteration 200 / 1500: loss 228.697209\n",
      "iteration 300 / 1500: loss 94.840741\n",
      "iteration 400 / 1500: loss 40.129359\n",
      "iteration 500 / 1500: loss 17.651949\n",
      "iteration 600 / 1500: loss 8.483781\n",
      "iteration 700 / 1500: loss 4.779775\n",
      "iteration 800 / 1500: loss 3.249482\n",
      "iteration 900 / 1500: loss 2.617445\n",
      "iteration 1000 / 1500: loss 2.305104\n",
      "iteration 1100 / 1500: loss 2.265029\n",
      "iteration 1200 / 1500: loss 2.169749\n",
      "iteration 1300 / 1500: loss 2.230091\n",
      "iteration 1400 / 1500: loss 2.131539\n",
      "iteration 0 / 1500: loss 1455.845386\n",
      "iteration 100 / 1500: loss 564.258908\n",
      "iteration 200 / 1500: loss 219.830562\n",
      "iteration 300 / 1500: loss 86.397497\n",
      "iteration 400 / 1500: loss 34.742634\n",
      "iteration 500 / 1500: loss 14.750192\n",
      "iteration 600 / 1500: loss 7.061512\n",
      "iteration 700 / 1500: loss 4.092263\n",
      "iteration 800 / 1500: loss 2.885949\n",
      "iteration 900 / 1500: loss 2.508005\n",
      "iteration 1000 / 1500: loss 2.261374\n",
      "iteration 1100 / 1500: loss 2.212494\n",
      "iteration 1200 / 1500: loss 2.157641\n",
      "iteration 1300 / 1500: loss 2.188776\n",
      "iteration 1400 / 1500: loss 2.245356\n",
      "iteration 0 / 1500: loss 1526.205833\n",
      "iteration 100 / 1500: loss 560.022190\n",
      "iteration 200 / 1500: loss 206.295861\n",
      "iteration 300 / 1500: loss 76.828043\n",
      "iteration 400 / 1500: loss 29.560090\n",
      "iteration 500 / 1500: loss 12.187797\n",
      "iteration 600 / 1500: loss 5.819584\n",
      "iteration 700 / 1500: loss 3.489695\n",
      "iteration 800 / 1500: loss 2.702745\n",
      "iteration 900 / 1500: loss 2.351006\n",
      "iteration 1000 / 1500: loss 2.282030\n",
      "iteration 1100 / 1500: loss 2.186944\n",
      "iteration 1200 / 1500: loss 2.214623\n",
      "iteration 1300 / 1500: loss 2.210040\n",
      "iteration 1400 / 1500: loss 2.181902\n",
      "iteration 0 / 1500: loss 778.641694\n",
      "iteration 100 / 1500: loss 376.423532\n",
      "iteration 200 / 1500: loss 183.215893\n",
      "iteration 300 / 1500: loss 89.580960\n",
      "iteration 400 / 1500: loss 44.495679\n",
      "iteration 500 / 1500: loss 22.541410\n",
      "iteration 600 / 1500: loss 12.028579\n",
      "iteration 700 / 1500: loss 6.900399\n",
      "iteration 800 / 1500: loss 4.460639\n",
      "iteration 900 / 1500: loss 3.251625\n",
      "iteration 1000 / 1500: loss 2.624879\n",
      "iteration 1100 / 1500: loss 2.369590\n",
      "iteration 1200 / 1500: loss 2.192635\n",
      "iteration 1300 / 1500: loss 2.169588\n",
      "iteration 1400 / 1500: loss 2.155000\n",
      "iteration 0 / 1500: loss 864.878644\n",
      "iteration 100 / 1500: loss 386.193746\n",
      "iteration 200 / 1500: loss 173.448136\n",
      "iteration 300 / 1500: loss 78.505087\n",
      "iteration 400 / 1500: loss 36.220526\n",
      "iteration 500 / 1500: loss 17.292252\n",
      "iteration 600 / 1500: loss 8.936857\n",
      "iteration 700 / 1500: loss 5.193973\n",
      "iteration 800 / 1500: loss 3.458145\n",
      "iteration 900 / 1500: loss 2.719078\n",
      "iteration 1000 / 1500: loss 2.374560\n",
      "iteration 1100 / 1500: loss 2.285233\n",
      "iteration 1200 / 1500: loss 2.132002\n",
      "iteration 1300 / 1500: loss 2.120318\n",
      "iteration 1400 / 1500: loss 2.133374\n",
      "iteration 0 / 1500: loss 941.869268\n",
      "iteration 100 / 1500: loss 388.040106\n",
      "iteration 200 / 1500: loss 160.822455\n",
      "iteration 300 / 1500: loss 67.358216\n",
      "iteration 400 / 1500: loss 28.907623\n",
      "iteration 500 / 1500: loss 13.181032\n",
      "iteration 600 / 1500: loss 6.727995\n",
      "iteration 700 / 1500: loss 4.039214\n",
      "iteration 800 / 1500: loss 2.939110\n",
      "iteration 900 / 1500: loss 2.375808\n",
      "iteration 1000 / 1500: loss 2.314218\n",
      "iteration 1100 / 1500: loss 2.206229\n",
      "iteration 1200 / 1500: loss 2.096629\n",
      "iteration 1300 / 1500: loss 2.173619\n",
      "iteration 1400 / 1500: loss 2.113960\n",
      "iteration 0 / 1500: loss 1022.285486\n",
      "iteration 100 / 1500: loss 388.483539\n",
      "iteration 200 / 1500: loss 148.644737\n",
      "iteration 300 / 1500: loss 57.758864\n",
      "iteration 400 / 1500: loss 23.240079\n",
      "iteration 500 / 1500: loss 10.192417\n",
      "iteration 600 / 1500: loss 5.140127\n",
      "iteration 700 / 1500: loss 3.288813\n",
      "iteration 800 / 1500: loss 2.585755\n",
      "iteration 900 / 1500: loss 2.331667\n",
      "iteration 1000 / 1500: loss 2.161906\n",
      "iteration 1100 / 1500: loss 2.209696\n",
      "iteration 1200 / 1500: loss 2.175790\n",
      "iteration 1300 / 1500: loss 2.140481\n",
      "iteration 1400 / 1500: loss 2.079811\n",
      "iteration 0 / 1500: loss 1100.630945\n",
      "iteration 100 / 1500: loss 386.243125\n",
      "iteration 200 / 1500: loss 136.573675\n",
      "iteration 300 / 1500: loss 49.361594\n",
      "iteration 400 / 1500: loss 18.623004\n",
      "iteration 500 / 1500: loss 7.894322\n",
      "iteration 600 / 1500: loss 4.244258\n",
      "iteration 700 / 1500: loss 2.894099\n",
      "iteration 800 / 1500: loss 2.419986\n",
      "iteration 900 / 1500: loss 2.206601\n",
      "iteration 1000 / 1500: loss 2.253040\n",
      "iteration 1100 / 1500: loss 2.145670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.174336\n",
      "iteration 1300 / 1500: loss 2.196460\n",
      "iteration 1400 / 1500: loss 2.195561\n",
      "iteration 0 / 1500: loss 1206.930991\n",
      "iteration 100 / 1500: loss 390.432090\n",
      "iteration 200 / 1500: loss 127.532850\n",
      "iteration 300 / 1500: loss 42.677123\n",
      "iteration 400 / 1500: loss 15.286152\n",
      "iteration 500 / 1500: loss 6.330265\n",
      "iteration 600 / 1500: loss 3.538968\n",
      "iteration 700 / 1500: loss 2.593963\n",
      "iteration 800 / 1500: loss 2.335896\n",
      "iteration 900 / 1500: loss 2.209039\n",
      "iteration 1000 / 1500: loss 2.128429\n",
      "iteration 1100 / 1500: loss 2.181268\n",
      "iteration 1200 / 1500: loss 2.091326\n",
      "iteration 1300 / 1500: loss 2.121680\n",
      "iteration 1400 / 1500: loss 2.151199\n",
      "iteration 0 / 1500: loss 1284.549339\n",
      "iteration 100 / 1500: loss 383.598704\n",
      "iteration 200 / 1500: loss 115.767800\n",
      "iteration 300 / 1500: loss 36.001700\n",
      "iteration 400 / 1500: loss 12.325281\n",
      "iteration 500 / 1500: loss 5.200125\n",
      "iteration 600 / 1500: loss 3.091091\n",
      "iteration 700 / 1500: loss 2.450689\n",
      "iteration 800 / 1500: loss 2.222111\n",
      "iteration 900 / 1500: loss 2.116221\n",
      "iteration 1000 / 1500: loss 2.142912\n",
      "iteration 1100 / 1500: loss 2.097812\n",
      "iteration 1200 / 1500: loss 2.136745\n",
      "iteration 1300 / 1500: loss 2.109882\n",
      "iteration 1400 / 1500: loss 2.195526\n",
      "iteration 0 / 1500: loss 1366.294323\n",
      "iteration 100 / 1500: loss 376.287804\n",
      "iteration 200 / 1500: loss 104.914836\n",
      "iteration 300 / 1500: loss 30.451510\n",
      "iteration 400 / 1500: loss 9.929977\n",
      "iteration 500 / 1500: loss 4.236342\n",
      "iteration 600 / 1500: loss 2.728678\n",
      "iteration 700 / 1500: loss 2.362603\n",
      "iteration 800 / 1500: loss 2.199860\n",
      "iteration 900 / 1500: loss 2.153123\n",
      "iteration 1000 / 1500: loss 2.237074\n",
      "iteration 1100 / 1500: loss 2.185598\n",
      "iteration 1200 / 1500: loss 2.086407\n",
      "iteration 1300 / 1500: loss 2.109392\n",
      "iteration 1400 / 1500: loss 2.088868\n",
      "iteration 0 / 1500: loss 1465.332267\n",
      "iteration 100 / 1500: loss 372.761558\n",
      "iteration 200 / 1500: loss 96.086968\n",
      "iteration 300 / 1500: loss 26.030965\n",
      "iteration 400 / 1500: loss 8.265365\n",
      "iteration 500 / 1500: loss 3.756737\n",
      "iteration 600 / 1500: loss 2.539271\n",
      "iteration 700 / 1500: loss 2.216098\n",
      "iteration 800 / 1500: loss 2.211331\n",
      "iteration 900 / 1500: loss 2.113971\n",
      "iteration 1000 / 1500: loss 2.130360\n",
      "iteration 1100 / 1500: loss 2.190060\n",
      "iteration 1200 / 1500: loss 2.134953\n",
      "iteration 1300 / 1500: loss 2.278465\n",
      "iteration 1400 / 1500: loss 2.136653\n",
      "iteration 0 / 1500: loss 1546.517709\n",
      "iteration 100 / 1500: loss 362.521047\n",
      "iteration 200 / 1500: loss 86.403141\n",
      "iteration 300 / 1500: loss 21.883043\n",
      "iteration 400 / 1500: loss 6.803445\n",
      "iteration 500 / 1500: loss 3.211642\n",
      "iteration 600 / 1500: loss 2.388888\n",
      "iteration 700 / 1500: loss 2.210744\n",
      "iteration 800 / 1500: loss 2.195536\n",
      "iteration 900 / 1500: loss 2.200419\n",
      "iteration 1000 / 1500: loss 2.180668\n",
      "iteration 1100 / 1500: loss 2.176356\n",
      "iteration 1200 / 1500: loss 2.157185\n",
      "iteration 1300 / 1500: loss 2.132091\n",
      "iteration 1400 / 1500: loss 2.140811\n",
      "iteration 0 / 1500: loss 775.401923\n",
      "iteration 100 / 1500: loss 300.063974\n",
      "iteration 200 / 1500: loss 117.157572\n",
      "iteration 300 / 1500: loss 46.678346\n",
      "iteration 400 / 1500: loss 19.259443\n",
      "iteration 500 / 1500: loss 8.783342\n",
      "iteration 600 / 1500: loss 4.714879\n",
      "iteration 700 / 1500: loss 3.106741\n",
      "iteration 800 / 1500: loss 2.506824\n",
      "iteration 900 / 1500: loss 2.250848\n",
      "iteration 1000 / 1500: loss 2.207566\n",
      "iteration 1100 / 1500: loss 2.159423\n",
      "iteration 1200 / 1500: loss 2.216265\n",
      "iteration 1300 / 1500: loss 2.102038\n",
      "iteration 1400 / 1500: loss 2.150091\n",
      "iteration 0 / 1500: loss 857.311080\n",
      "iteration 100 / 1500: loss 298.674273\n",
      "iteration 200 / 1500: loss 105.229998\n",
      "iteration 300 / 1500: loss 38.008331\n",
      "iteration 400 / 1500: loss 14.642987\n",
      "iteration 500 / 1500: loss 6.487836\n",
      "iteration 600 / 1500: loss 3.612368\n",
      "iteration 700 / 1500: loss 2.598088\n",
      "iteration 800 / 1500: loss 2.295391\n",
      "iteration 900 / 1500: loss 2.194873\n",
      "iteration 1000 / 1500: loss 2.138947\n",
      "iteration 1100 / 1500: loss 2.091993\n",
      "iteration 1200 / 1500: loss 2.060427\n",
      "iteration 1300 / 1500: loss 2.137247\n",
      "iteration 1400 / 1500: loss 2.089553\n",
      "iteration 0 / 1500: loss 938.522223\n",
      "iteration 100 / 1500: loss 294.552587\n",
      "iteration 200 / 1500: loss 93.662202\n",
      "iteration 300 / 1500: loss 30.704396\n",
      "iteration 400 / 1500: loss 11.045321\n",
      "iteration 500 / 1500: loss 4.990157\n",
      "iteration 600 / 1500: loss 3.025150\n",
      "iteration 700 / 1500: loss 2.362322\n",
      "iteration 800 / 1500: loss 2.204778\n",
      "iteration 900 / 1500: loss 2.143259\n",
      "iteration 1000 / 1500: loss 2.139113\n",
      "iteration 1100 / 1500: loss 2.146039\n",
      "iteration 1200 / 1500: loss 2.126156\n",
      "iteration 1300 / 1500: loss 2.119384\n",
      "iteration 1400 / 1500: loss 2.109133\n",
      "iteration 0 / 1500: loss 1029.895996\n",
      "iteration 100 / 1500: loss 290.598700\n",
      "iteration 200 / 1500: loss 83.438773\n",
      "iteration 300 / 1500: loss 25.020580\n",
      "iteration 400 / 1500: loss 8.532061\n",
      "iteration 500 / 1500: loss 3.891893\n",
      "iteration 600 / 1500: loss 2.691948\n",
      "iteration 700 / 1500: loss 2.255883\n",
      "iteration 800 / 1500: loss 2.257558\n",
      "iteration 900 / 1500: loss 2.187818\n",
      "iteration 1000 / 1500: loss 2.132838\n",
      "iteration 1100 / 1500: loss 2.122439\n",
      "iteration 1200 / 1500: loss 2.074682\n",
      "iteration 1300 / 1500: loss 2.125194\n",
      "iteration 1400 / 1500: loss 2.151577\n",
      "iteration 0 / 1500: loss 1110.801547\n",
      "iteration 100 / 1500: loss 282.127278\n",
      "iteration 200 / 1500: loss 73.080195\n",
      "iteration 300 / 1500: loss 20.150531\n",
      "iteration 400 / 1500: loss 6.695225\n",
      "iteration 500 / 1500: loss 3.290692\n",
      "iteration 600 / 1500: loss 2.394383\n",
      "iteration 700 / 1500: loss 2.223429\n",
      "iteration 800 / 1500: loss 2.165280\n",
      "iteration 900 / 1500: loss 2.150175\n",
      "iteration 1000 / 1500: loss 2.099670\n",
      "iteration 1100 / 1500: loss 2.122613\n",
      "iteration 1200 / 1500: loss 2.164744\n",
      "iteration 1300 / 1500: loss 2.112397\n",
      "iteration 1400 / 1500: loss 2.118705\n",
      "iteration 0 / 1500: loss 1200.105103\n",
      "iteration 100 / 1500: loss 274.646924\n",
      "iteration 200 / 1500: loss 64.387002\n",
      "iteration 300 / 1500: loss 16.261453\n",
      "iteration 400 / 1500: loss 5.374600\n",
      "iteration 500 / 1500: loss 2.887772\n",
      "iteration 600 / 1500: loss 2.253756\n",
      "iteration 700 / 1500: loss 2.174521\n",
      "iteration 800 / 1500: loss 2.144945\n",
      "iteration 900 / 1500: loss 2.162020\n",
      "iteration 1000 / 1500: loss 2.202209\n",
      "iteration 1100 / 1500: loss 2.151877\n",
      "iteration 1200 / 1500: loss 2.208345\n",
      "iteration 1300 / 1500: loss 2.149598\n",
      "iteration 1400 / 1500: loss 2.156355\n",
      "iteration 0 / 1500: loss 1281.540840\n",
      "iteration 100 / 1500: loss 264.068786\n",
      "iteration 200 / 1500: loss 55.781432\n",
      "iteration 300 / 1500: loss 13.139783\n",
      "iteration 400 / 1500: loss 4.473289\n",
      "iteration 500 / 1500: loss 2.625113\n",
      "iteration 600 / 1500: loss 2.183207\n",
      "iteration 700 / 1500: loss 2.187320\n",
      "iteration 800 / 1500: loss 2.205503\n",
      "iteration 900 / 1500: loss 2.185653\n",
      "iteration 1000 / 1500: loss 2.165027\n",
      "iteration 1100 / 1500: loss 2.147821\n",
      "iteration 1200 / 1500: loss 2.171986\n",
      "iteration 1300 / 1500: loss 2.203499\n",
      "iteration 1400 / 1500: loss 2.262785\n",
      "iteration 0 / 1500: loss 1371.140757\n",
      "iteration 100 / 1500: loss 254.232076\n",
      "iteration 200 / 1500: loss 48.617168\n",
      "iteration 300 / 1500: loss 10.706280\n",
      "iteration 400 / 1500: loss 3.718495\n",
      "iteration 500 / 1500: loss 2.451360\n",
      "iteration 600 / 1500: loss 2.202159\n",
      "iteration 700 / 1500: loss 2.172755\n",
      "iteration 800 / 1500: loss 2.155022\n",
      "iteration 900 / 1500: loss 2.148689\n",
      "iteration 1000 / 1500: loss 2.162904\n",
      "iteration 1100 / 1500: loss 2.215856\n",
      "iteration 1200 / 1500: loss 2.133420\n",
      "iteration 1300 / 1500: loss 2.180791\n",
      "iteration 1400 / 1500: loss 2.192123\n",
      "iteration 0 / 1500: loss 1446.903857\n",
      "iteration 100 / 1500: loss 241.329998\n",
      "iteration 200 / 1500: loss 41.856658\n",
      "iteration 300 / 1500: loss 8.819509\n",
      "iteration 400 / 1500: loss 3.223128\n",
      "iteration 500 / 1500: loss 2.335379\n",
      "iteration 600 / 1500: loss 2.222926\n",
      "iteration 700 / 1500: loss 2.218242\n",
      "iteration 800 / 1500: loss 2.162457\n",
      "iteration 900 / 1500: loss 2.232084\n",
      "iteration 1000 / 1500: loss 2.233316\n",
      "iteration 1100 / 1500: loss 2.099533\n",
      "iteration 1200 / 1500: loss 2.208122\n",
      "iteration 1300 / 1500: loss 2.196681\n",
      "iteration 1400 / 1500: loss 2.159436\n",
      "iteration 0 / 1500: loss 1548.657933\n",
      "iteration 100 / 1500: loss 232.527957\n",
      "iteration 200 / 1500: loss 36.538569\n",
      "iteration 300 / 1500: loss 7.341713\n",
      "iteration 400 / 1500: loss 2.916321\n",
      "iteration 500 / 1500: loss 2.355812\n",
      "iteration 600 / 1500: loss 2.224229\n",
      "iteration 700 / 1500: loss 2.170754\n",
      "iteration 800 / 1500: loss 2.125906\n",
      "iteration 900 / 1500: loss 2.141915\n",
      "iteration 1000 / 1500: loss 2.225992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 2.149227\n",
      "iteration 1200 / 1500: loss 2.229100\n",
      "iteration 1300 / 1500: loss 2.120472\n",
      "iteration 1400 / 1500: loss 2.203010\n",
      "iteration 0 / 1500: loss 764.225375\n",
      "iteration 100 / 1500: loss 236.916757\n",
      "iteration 200 / 1500: loss 74.724469\n",
      "iteration 300 / 1500: loss 24.427677\n",
      "iteration 400 / 1500: loss 9.047813\n",
      "iteration 500 / 1500: loss 4.266115\n",
      "iteration 600 / 1500: loss 2.782401\n",
      "iteration 700 / 1500: loss 2.274925\n",
      "iteration 800 / 1500: loss 2.212529\n",
      "iteration 900 / 1500: loss 2.117870\n",
      "iteration 1000 / 1500: loss 2.108252\n",
      "iteration 1100 / 1500: loss 2.095559\n",
      "iteration 1200 / 1500: loss 2.041518\n",
      "iteration 1300 / 1500: loss 2.103965\n",
      "iteration 1400 / 1500: loss 2.101634\n",
      "iteration 0 / 1500: loss 849.891991\n",
      "iteration 100 / 1500: loss 231.262792\n",
      "iteration 200 / 1500: loss 64.118506\n",
      "iteration 300 / 1500: loss 18.927066\n",
      "iteration 400 / 1500: loss 6.679734\n",
      "iteration 500 / 1500: loss 3.422755\n",
      "iteration 600 / 1500: loss 2.363187\n",
      "iteration 700 / 1500: loss 2.168995\n",
      "iteration 800 / 1500: loss 2.164194\n",
      "iteration 900 / 1500: loss 2.111920\n",
      "iteration 1000 / 1500: loss 2.106767\n",
      "iteration 1100 / 1500: loss 2.067575\n",
      "iteration 1200 / 1500: loss 2.058346\n",
      "iteration 1300 / 1500: loss 2.105411\n",
      "iteration 1400 / 1500: loss 2.107571\n",
      "iteration 0 / 1500: loss 945.145517\n",
      "iteration 100 / 1500: loss 226.036407\n",
      "iteration 200 / 1500: loss 55.398692\n",
      "iteration 300 / 1500: loss 14.814663\n",
      "iteration 400 / 1500: loss 5.136478\n",
      "iteration 500 / 1500: loss 2.875979\n",
      "iteration 600 / 1500: loss 2.319577\n",
      "iteration 700 / 1500: loss 2.146867\n",
      "iteration 800 / 1500: loss 2.095154\n",
      "iteration 900 / 1500: loss 2.082914\n",
      "iteration 1000 / 1500: loss 2.090021\n",
      "iteration 1100 / 1500: loss 2.156760\n",
      "iteration 1200 / 1500: loss 2.092521\n",
      "iteration 1300 / 1500: loss 2.160644\n",
      "iteration 1400 / 1500: loss 2.132794\n",
      "iteration 0 / 1500: loss 1021.867772\n",
      "iteration 100 / 1500: loss 214.518089\n",
      "iteration 200 / 1500: loss 46.471204\n",
      "iteration 300 / 1500: loss 11.350202\n",
      "iteration 400 / 1500: loss 4.089919\n",
      "iteration 500 / 1500: loss 2.534746\n",
      "iteration 600 / 1500: loss 2.222896\n",
      "iteration 700 / 1500: loss 2.132140\n",
      "iteration 800 / 1500: loss 2.192624\n",
      "iteration 900 / 1500: loss 2.106738\n",
      "iteration 1000 / 1500: loss 2.155102\n",
      "iteration 1100 / 1500: loss 2.091672\n",
      "iteration 1200 / 1500: loss 2.109631\n",
      "iteration 1300 / 1500: loss 2.094986\n",
      "iteration 1400 / 1500: loss 2.087718\n",
      "iteration 0 / 1500: loss 1108.982007\n",
      "iteration 100 / 1500: loss 204.208711\n",
      "iteration 200 / 1500: loss 39.062804\n",
      "iteration 300 / 1500: loss 8.991919\n",
      "iteration 400 / 1500: loss 3.398411\n",
      "iteration 500 / 1500: loss 2.312364\n",
      "iteration 600 / 1500: loss 2.163299\n",
      "iteration 700 / 1500: loss 2.178532\n",
      "iteration 800 / 1500: loss 2.161615\n",
      "iteration 900 / 1500: loss 2.109831\n",
      "iteration 1000 / 1500: loss 2.111495\n",
      "iteration 1100 / 1500: loss 2.183523\n",
      "iteration 1200 / 1500: loss 2.178343\n",
      "iteration 1300 / 1500: loss 2.173814\n",
      "iteration 1400 / 1500: loss 2.110169\n",
      "iteration 0 / 1500: loss 1201.616339\n",
      "iteration 100 / 1500: loss 194.486498\n",
      "iteration 200 / 1500: loss 33.017352\n",
      "iteration 300 / 1500: loss 7.190514\n",
      "iteration 400 / 1500: loss 3.001625\n",
      "iteration 500 / 1500: loss 2.294434\n",
      "iteration 600 / 1500: loss 2.208355\n",
      "iteration 700 / 1500: loss 2.115108\n",
      "iteration 800 / 1500: loss 2.094685\n",
      "iteration 900 / 1500: loss 2.165103\n",
      "iteration 1000 / 1500: loss 2.167842\n",
      "iteration 1100 / 1500: loss 2.159460\n",
      "iteration 1200 / 1500: loss 2.135947\n",
      "iteration 1300 / 1500: loss 2.188464\n",
      "iteration 1400 / 1500: loss 2.159832\n",
      "iteration 0 / 1500: loss 1298.447162\n",
      "iteration 100 / 1500: loss 184.339145\n",
      "iteration 200 / 1500: loss 27.883184\n",
      "iteration 300 / 1500: loss 5.811371\n",
      "iteration 400 / 1500: loss 2.711224\n",
      "iteration 500 / 1500: loss 2.239997\n",
      "iteration 600 / 1500: loss 2.270870\n",
      "iteration 700 / 1500: loss 2.212598\n",
      "iteration 800 / 1500: loss 2.234356\n",
      "iteration 900 / 1500: loss 2.130879\n",
      "iteration 1000 / 1500: loss 2.177354\n",
      "iteration 1100 / 1500: loss 2.108868\n",
      "iteration 1200 / 1500: loss 2.093292\n",
      "iteration 1300 / 1500: loss 2.194244\n",
      "iteration 1400 / 1500: loss 2.192833\n",
      "iteration 0 / 1500: loss 1356.296118\n",
      "iteration 100 / 1500: loss 169.065394\n",
      "iteration 200 / 1500: loss 22.798852\n",
      "iteration 300 / 1500: loss 4.686547\n",
      "iteration 400 / 1500: loss 2.418952\n",
      "iteration 500 / 1500: loss 2.199234\n",
      "iteration 600 / 1500: loss 2.113594\n",
      "iteration 700 / 1500: loss 2.232973\n",
      "iteration 800 / 1500: loss 2.207313\n",
      "iteration 900 / 1500: loss 2.116135\n",
      "iteration 1000 / 1500: loss 2.144045\n",
      "iteration 1100 / 1500: loss 2.154968\n",
      "iteration 1200 / 1500: loss 2.203806\n",
      "iteration 1300 / 1500: loss 2.165132\n",
      "iteration 1400 / 1500: loss 2.146071\n",
      "iteration 0 / 1500: loss 1456.481043\n",
      "iteration 100 / 1500: loss 159.658892\n",
      "iteration 200 / 1500: loss 19.277504\n",
      "iteration 300 / 1500: loss 4.056372\n",
      "iteration 400 / 1500: loss 2.367071\n",
      "iteration 500 / 1500: loss 2.185803\n",
      "iteration 600 / 1500: loss 2.175661\n",
      "iteration 700 / 1500: loss 2.178656\n",
      "iteration 800 / 1500: loss 2.200684\n",
      "iteration 900 / 1500: loss 2.100715\n",
      "iteration 1000 / 1500: loss 2.142069\n",
      "iteration 1100 / 1500: loss 2.120419\n",
      "iteration 1200 / 1500: loss 2.138048\n",
      "iteration 1300 / 1500: loss 2.148043\n",
      "iteration 1400 / 1500: loss 2.119435\n",
      "iteration 0 / 1500: loss 1541.743224\n",
      "iteration 100 / 1500: loss 148.236651\n",
      "iteration 200 / 1500: loss 16.071689\n",
      "iteration 300 / 1500: loss 3.496211\n",
      "iteration 400 / 1500: loss 2.321670\n",
      "iteration 500 / 1500: loss 2.206018\n",
      "iteration 600 / 1500: loss 2.204741\n",
      "iteration 700 / 1500: loss 2.148668\n",
      "iteration 800 / 1500: loss 2.156942\n",
      "iteration 900 / 1500: loss 2.137570\n",
      "iteration 1000 / 1500: loss 2.170861\n",
      "iteration 1100 / 1500: loss 2.127158\n",
      "iteration 1200 / 1500: loss 2.164841\n",
      "iteration 1300 / 1500: loss 2.188822\n",
      "iteration 1400 / 1500: loss 2.157247\n",
      "iteration 0 / 1500: loss 783.750099\n",
      "iteration 100 / 1500: loss 194.381301\n",
      "iteration 200 / 1500: loss 49.502762\n",
      "iteration 300 / 1500: loss 13.846507\n",
      "iteration 400 / 1500: loss 5.065169\n",
      "iteration 500 / 1500: loss 2.863262\n",
      "iteration 600 / 1500: loss 2.293968\n",
      "iteration 700 / 1500: loss 2.184679\n",
      "iteration 800 / 1500: loss 2.119294\n",
      "iteration 900 / 1500: loss 2.157883\n",
      "iteration 1000 / 1500: loss 2.071764\n",
      "iteration 1100 / 1500: loss 2.134330\n",
      "iteration 1200 / 1500: loss 2.118473\n",
      "iteration 1300 / 1500: loss 2.029204\n",
      "iteration 1400 / 1500: loss 2.104073\n",
      "iteration 0 / 1500: loss 855.752038\n",
      "iteration 100 / 1500: loss 181.939964\n",
      "iteration 200 / 1500: loss 40.178458\n",
      "iteration 300 / 1500: loss 10.135179\n",
      "iteration 400 / 1500: loss 3.785293\n",
      "iteration 500 / 1500: loss 2.509202\n",
      "iteration 600 / 1500: loss 2.235732\n",
      "iteration 700 / 1500: loss 2.134044\n",
      "iteration 800 / 1500: loss 2.125861\n",
      "iteration 900 / 1500: loss 2.095411\n",
      "iteration 1000 / 1500: loss 2.123183\n",
      "iteration 1100 / 1500: loss 2.131614\n",
      "iteration 1200 / 1500: loss 2.143407\n",
      "iteration 1300 / 1500: loss 2.110420\n",
      "iteration 1400 / 1500: loss 2.080363\n",
      "iteration 0 / 1500: loss 934.712082\n",
      "iteration 100 / 1500: loss 170.216476\n",
      "iteration 200 / 1500: loss 32.478968\n",
      "iteration 300 / 1500: loss 7.651726\n",
      "iteration 400 / 1500: loss 3.189610\n",
      "iteration 500 / 1500: loss 2.288417\n",
      "iteration 600 / 1500: loss 2.138642\n",
      "iteration 700 / 1500: loss 2.128221\n",
      "iteration 800 / 1500: loss 2.094869\n",
      "iteration 900 / 1500: loss 2.060251\n",
      "iteration 1000 / 1500: loss 2.105850\n",
      "iteration 1100 / 1500: loss 2.153897\n",
      "iteration 1200 / 1500: loss 2.207638\n",
      "iteration 1300 / 1500: loss 2.114772\n",
      "iteration 1400 / 1500: loss 2.112382\n",
      "iteration 0 / 1500: loss 1033.568660\n",
      "iteration 100 / 1500: loss 161.211024\n",
      "iteration 200 / 1500: loss 26.734010\n",
      "iteration 300 / 1500: loss 5.945598\n",
      "iteration 400 / 1500: loss 2.747049\n",
      "iteration 500 / 1500: loss 2.182956\n",
      "iteration 600 / 1500: loss 2.170828\n",
      "iteration 700 / 1500: loss 2.167634\n",
      "iteration 800 / 1500: loss 2.175569\n",
      "iteration 900 / 1500: loss 2.133130\n",
      "iteration 1000 / 1500: loss 2.135198\n",
      "iteration 1100 / 1500: loss 2.196738\n",
      "iteration 1200 / 1500: loss 2.099205\n",
      "iteration 1300 / 1500: loss 2.111511\n",
      "iteration 1400 / 1500: loss 2.170206\n",
      "iteration 0 / 1500: loss 1110.110816\n",
      "iteration 100 / 1500: loss 148.160181\n",
      "iteration 200 / 1500: loss 21.496578\n",
      "iteration 300 / 1500: loss 4.699883\n",
      "iteration 400 / 1500: loss 2.479338\n",
      "iteration 500 / 1500: loss 2.224501\n",
      "iteration 600 / 1500: loss 2.143382\n",
      "iteration 700 / 1500: loss 2.181560\n",
      "iteration 800 / 1500: loss 2.075805\n",
      "iteration 900 / 1500: loss 2.150404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.132010\n",
      "iteration 1100 / 1500: loss 2.131232\n",
      "iteration 1200 / 1500: loss 2.191061\n",
      "iteration 1300 / 1500: loss 2.250158\n",
      "iteration 1400 / 1500: loss 2.215503\n",
      "iteration 0 / 1500: loss 1214.735893\n",
      "iteration 100 / 1500: loss 139.153453\n",
      "iteration 200 / 1500: loss 17.685710\n",
      "iteration 300 / 1500: loss 3.884941\n",
      "iteration 400 / 1500: loss 2.300353\n",
      "iteration 500 / 1500: loss 2.194573\n",
      "iteration 600 / 1500: loss 2.202745\n",
      "iteration 700 / 1500: loss 2.132625\n",
      "iteration 800 / 1500: loss 2.107778\n",
      "iteration 900 / 1500: loss 2.149368\n",
      "iteration 1000 / 1500: loss 2.130263\n",
      "iteration 1100 / 1500: loss 2.132138\n",
      "iteration 1200 / 1500: loss 2.155318\n",
      "iteration 1300 / 1500: loss 2.095556\n",
      "iteration 1400 / 1500: loss 2.173440\n",
      "iteration 0 / 1500: loss 1267.026269\n",
      "iteration 100 / 1500: loss 124.498806\n",
      "iteration 200 / 1500: loss 14.016646\n",
      "iteration 300 / 1500: loss 3.345859\n",
      "iteration 400 / 1500: loss 2.269692\n",
      "iteration 500 / 1500: loss 2.154841\n",
      "iteration 600 / 1500: loss 2.140720\n",
      "iteration 700 / 1500: loss 2.174420\n",
      "iteration 800 / 1500: loss 2.100547\n",
      "iteration 900 / 1500: loss 2.166401\n",
      "iteration 1000 / 1500: loss 2.223821\n",
      "iteration 1100 / 1500: loss 2.191687\n",
      "iteration 1200 / 1500: loss 2.090261\n",
      "iteration 1300 / 1500: loss 2.137492\n",
      "iteration 1400 / 1500: loss 2.205088\n",
      "iteration 0 / 1500: loss 1381.368940\n",
      "iteration 100 / 1500: loss 116.279798\n",
      "iteration 200 / 1500: loss 11.607665\n",
      "iteration 300 / 1500: loss 2.981532\n",
      "iteration 400 / 1500: loss 2.228141\n",
      "iteration 500 / 1500: loss 2.194435\n",
      "iteration 600 / 1500: loss 2.139415\n",
      "iteration 700 / 1500: loss 2.216668\n",
      "iteration 800 / 1500: loss 2.145130\n",
      "iteration 900 / 1500: loss 2.190652\n",
      "iteration 1000 / 1500: loss 2.100943\n",
      "iteration 1100 / 1500: loss 2.179483\n",
      "iteration 1200 / 1500: loss 2.169156\n",
      "iteration 1300 / 1500: loss 2.135244\n",
      "iteration 1400 / 1500: loss 2.146165\n",
      "iteration 0 / 1500: loss 1457.826668\n",
      "iteration 100 / 1500: loss 105.140284\n",
      "iteration 200 / 1500: loss 9.406128\n",
      "iteration 300 / 1500: loss 2.691627\n",
      "iteration 400 / 1500: loss 2.176360\n",
      "iteration 500 / 1500: loss 2.147639\n",
      "iteration 600 / 1500: loss 2.138168\n",
      "iteration 700 / 1500: loss 2.193995\n",
      "iteration 800 / 1500: loss 2.119251\n",
      "iteration 900 / 1500: loss 2.202036\n",
      "iteration 1000 / 1500: loss 2.214504\n",
      "iteration 1100 / 1500: loss 2.218014\n",
      "iteration 1200 / 1500: loss 2.127140\n",
      "iteration 1300 / 1500: loss 2.192929\n",
      "iteration 1400 / 1500: loss 2.149493\n",
      "iteration 0 / 1500: loss 1530.999115\n",
      "iteration 100 / 1500: loss 94.675991\n",
      "iteration 200 / 1500: loss 7.751384\n",
      "iteration 300 / 1500: loss 2.486285\n",
      "iteration 400 / 1500: loss 2.201742\n",
      "iteration 500 / 1500: loss 2.160766\n",
      "iteration 600 / 1500: loss 2.159766\n",
      "iteration 700 / 1500: loss 2.174982\n",
      "iteration 800 / 1500: loss 2.193816\n",
      "iteration 900 / 1500: loss 2.143758\n",
      "iteration 1000 / 1500: loss 2.124511\n",
      "iteration 1100 / 1500: loss 2.177949\n",
      "iteration 1200 / 1500: loss 2.153701\n",
      "iteration 1300 / 1500: loss 2.184437\n",
      "iteration 1400 / 1500: loss 2.242315\n",
      "iteration 0 / 1500: loss 766.474554\n",
      "iteration 100 / 1500: loss 152.307546\n",
      "iteration 200 / 1500: loss 31.682064\n",
      "iteration 300 / 1500: loss 7.972337\n",
      "iteration 400 / 1500: loss 3.331044\n",
      "iteration 500 / 1500: loss 2.363237\n",
      "iteration 600 / 1500: loss 2.213940\n",
      "iteration 700 / 1500: loss 2.115707\n",
      "iteration 800 / 1500: loss 2.099023\n",
      "iteration 900 / 1500: loss 2.075269\n",
      "iteration 1000 / 1500: loss 2.161412\n",
      "iteration 1100 / 1500: loss 2.148867\n",
      "iteration 1200 / 1500: loss 2.130902\n",
      "iteration 1300 / 1500: loss 2.134481\n",
      "iteration 1400 / 1500: loss 2.157207\n",
      "iteration 0 / 1500: loss 852.409071\n",
      "iteration 100 / 1500: loss 141.635418\n",
      "iteration 200 / 1500: loss 25.061097\n",
      "iteration 300 / 1500: loss 5.882107\n",
      "iteration 400 / 1500: loss 2.766706\n",
      "iteration 500 / 1500: loss 2.258913\n",
      "iteration 600 / 1500: loss 2.152445\n",
      "iteration 700 / 1500: loss 2.140850\n",
      "iteration 800 / 1500: loss 2.193093\n",
      "iteration 900 / 1500: loss 2.106627\n",
      "iteration 1000 / 1500: loss 2.138475\n",
      "iteration 1100 / 1500: loss 2.117825\n",
      "iteration 1200 / 1500: loss 2.136876\n",
      "iteration 1300 / 1500: loss 2.135093\n",
      "iteration 1400 / 1500: loss 2.115182\n",
      "iteration 0 / 1500: loss 948.423148\n",
      "iteration 100 / 1500: loss 131.386719\n",
      "iteration 200 / 1500: loss 19.878629\n",
      "iteration 300 / 1500: loss 4.600555\n",
      "iteration 400 / 1500: loss 2.422566\n",
      "iteration 500 / 1500: loss 2.142535\n",
      "iteration 600 / 1500: loss 2.067154\n",
      "iteration 700 / 1500: loss 2.170242\n",
      "iteration 800 / 1500: loss 2.184290\n",
      "iteration 900 / 1500: loss 2.178753\n",
      "iteration 1000 / 1500: loss 2.114353\n",
      "iteration 1100 / 1500: loss 2.149994\n",
      "iteration 1200 / 1500: loss 2.125928\n",
      "iteration 1300 / 1500: loss 2.069566\n",
      "iteration 1400 / 1500: loss 2.132510\n",
      "iteration 0 / 1500: loss 1038.811895\n",
      "iteration 100 / 1500: loss 120.602051\n",
      "iteration 200 / 1500: loss 15.695897\n",
      "iteration 300 / 1500: loss 3.644054\n",
      "iteration 400 / 1500: loss 2.299581\n",
      "iteration 500 / 1500: loss 2.149876\n",
      "iteration 600 / 1500: loss 2.196314\n",
      "iteration 700 / 1500: loss 2.107006\n",
      "iteration 800 / 1500: loss 2.165784\n",
      "iteration 900 / 1500: loss 2.129695\n",
      "iteration 1000 / 1500: loss 2.125427\n",
      "iteration 1100 / 1500: loss 2.182089\n",
      "iteration 1200 / 1500: loss 2.204879\n",
      "iteration 1300 / 1500: loss 2.124087\n",
      "iteration 1400 / 1500: loss 2.129602\n",
      "iteration 0 / 1500: loss 1121.279237\n",
      "iteration 100 / 1500: loss 108.811085\n",
      "iteration 200 / 1500: loss 12.415527\n",
      "iteration 300 / 1500: loss 3.092422\n",
      "iteration 400 / 1500: loss 2.230427\n",
      "iteration 500 / 1500: loss 2.142619\n",
      "iteration 600 / 1500: loss 2.179358\n",
      "iteration 700 / 1500: loss 2.096528\n",
      "iteration 800 / 1500: loss 2.152948\n",
      "iteration 900 / 1500: loss 2.138333\n",
      "iteration 1000 / 1500: loss 2.220284\n",
      "iteration 1100 / 1500: loss 2.175916\n",
      "iteration 1200 / 1500: loss 2.193179\n",
      "iteration 1300 / 1500: loss 2.101273\n",
      "iteration 1400 / 1500: loss 2.189020\n",
      "iteration 0 / 1500: loss 1202.208015\n",
      "iteration 100 / 1500: loss 97.619364\n",
      "iteration 200 / 1500: loss 9.791310\n",
      "iteration 300 / 1500: loss 2.823385\n",
      "iteration 400 / 1500: loss 2.107619\n",
      "iteration 500 / 1500: loss 2.151148\n",
      "iteration 600 / 1500: loss 2.125084\n",
      "iteration 700 / 1500: loss 2.132500\n",
      "iteration 800 / 1500: loss 2.115163\n",
      "iteration 900 / 1500: loss 2.132974\n",
      "iteration 1000 / 1500: loss 2.219366\n",
      "iteration 1100 / 1500: loss 2.187314\n",
      "iteration 1200 / 1500: loss 2.117359\n",
      "iteration 1300 / 1500: loss 2.158303\n",
      "iteration 1400 / 1500: loss 2.159565\n",
      "iteration 0 / 1500: loss 1282.372394\n",
      "iteration 100 / 1500: loss 87.001403\n",
      "iteration 200 / 1500: loss 7.839346\n",
      "iteration 300 / 1500: loss 2.596670\n",
      "iteration 400 / 1500: loss 2.201167\n",
      "iteration 500 / 1500: loss 2.271476\n",
      "iteration 600 / 1500: loss 2.122481\n",
      "iteration 700 / 1500: loss 2.133737\n",
      "iteration 800 / 1500: loss 2.118665\n",
      "iteration 900 / 1500: loss 2.177938\n",
      "iteration 1000 / 1500: loss 2.177463\n",
      "iteration 1100 / 1500: loss 2.192124\n",
      "iteration 1200 / 1500: loss 2.144905\n",
      "iteration 1300 / 1500: loss 2.151128\n",
      "iteration 1400 / 1500: loss 2.140932\n",
      "iteration 0 / 1500: loss 1376.980094\n",
      "iteration 100 / 1500: loss 78.302853\n",
      "iteration 200 / 1500: loss 6.371375\n",
      "iteration 300 / 1500: loss 2.450070\n",
      "iteration 400 / 1500: loss 2.169341\n",
      "iteration 500 / 1500: loss 2.150214\n",
      "iteration 600 / 1500: loss 2.147320\n",
      "iteration 700 / 1500: loss 2.244460\n",
      "iteration 800 / 1500: loss 2.078182\n",
      "iteration 900 / 1500: loss 2.172057\n",
      "iteration 1000 / 1500: loss 2.148503\n",
      "iteration 1100 / 1500: loss 2.210620\n",
      "iteration 1200 / 1500: loss 2.237542\n",
      "iteration 1300 / 1500: loss 2.196489\n",
      "iteration 1400 / 1500: loss 2.159696\n",
      "iteration 0 / 1500: loss 1449.261504\n",
      "iteration 100 / 1500: loss 69.026765\n",
      "iteration 200 / 1500: loss 5.238513\n",
      "iteration 300 / 1500: loss 2.286199\n",
      "iteration 400 / 1500: loss 2.127305\n",
      "iteration 500 / 1500: loss 2.193729\n",
      "iteration 600 / 1500: loss 2.189210\n",
      "iteration 700 / 1500: loss 2.211905\n",
      "iteration 800 / 1500: loss 2.200715\n",
      "iteration 900 / 1500: loss 2.148751\n",
      "iteration 1000 / 1500: loss 2.190233\n",
      "iteration 1100 / 1500: loss 2.177545\n",
      "iteration 1200 / 1500: loss 2.145592\n",
      "iteration 1300 / 1500: loss 2.151097\n",
      "iteration 1400 / 1500: loss 2.129596\n",
      "iteration 0 / 1500: loss 1553.051115\n",
      "iteration 100 / 1500: loss 61.838712\n",
      "iteration 200 / 1500: loss 4.433194\n",
      "iteration 300 / 1500: loss 2.240066\n",
      "iteration 400 / 1500: loss 2.201094\n",
      "iteration 500 / 1500: loss 2.182020\n",
      "iteration 600 / 1500: loss 2.226471\n",
      "iteration 700 / 1500: loss 2.183853\n",
      "iteration 800 / 1500: loss 2.181027\n",
      "iteration 900 / 1500: loss 2.176021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.213352\n",
      "iteration 1100 / 1500: loss 2.237022\n",
      "iteration 1200 / 1500: loss 2.159075\n",
      "iteration 1300 / 1500: loss 2.149688\n",
      "iteration 1400 / 1500: loss 2.142036\n",
      "iteration 0 / 1500: loss 772.242138\n",
      "iteration 100 / 1500: loss 122.998873\n",
      "iteration 200 / 1500: loss 21.134573\n",
      "iteration 300 / 1500: loss 5.089352\n",
      "iteration 400 / 1500: loss 2.615104\n",
      "iteration 500 / 1500: loss 2.186021\n",
      "iteration 600 / 1500: loss 2.097350\n",
      "iteration 700 / 1500: loss 2.080132\n",
      "iteration 800 / 1500: loss 2.117519\n",
      "iteration 900 / 1500: loss 2.182359\n",
      "iteration 1000 / 1500: loss 2.107522\n",
      "iteration 1100 / 1500: loss 2.064542\n",
      "iteration 1200 / 1500: loss 2.121225\n",
      "iteration 1300 / 1500: loss 2.041248\n",
      "iteration 1400 / 1500: loss 2.186525\n",
      "iteration 0 / 1500: loss 849.772610\n",
      "iteration 100 / 1500: loss 110.357417\n",
      "iteration 200 / 1500: loss 16.020336\n",
      "iteration 300 / 1500: loss 3.899508\n",
      "iteration 400 / 1500: loss 2.352316\n",
      "iteration 500 / 1500: loss 2.168129\n",
      "iteration 600 / 1500: loss 2.080679\n",
      "iteration 700 / 1500: loss 2.107526\n",
      "iteration 800 / 1500: loss 2.142033\n",
      "iteration 900 / 1500: loss 2.134499\n",
      "iteration 1000 / 1500: loss 2.158027\n",
      "iteration 1100 / 1500: loss 2.135878\n",
      "iteration 1200 / 1500: loss 2.233020\n",
      "iteration 1300 / 1500: loss 2.088932\n",
      "iteration 1400 / 1500: loss 2.202415\n",
      "iteration 0 / 1500: loss 951.360556\n",
      "iteration 100 / 1500: loss 100.751319\n",
      "iteration 200 / 1500: loss 12.368101\n",
      "iteration 300 / 1500: loss 3.117079\n",
      "iteration 400 / 1500: loss 2.242457\n",
      "iteration 500 / 1500: loss 2.174053\n",
      "iteration 600 / 1500: loss 2.147306\n",
      "iteration 700 / 1500: loss 2.137563\n",
      "iteration 800 / 1500: loss 2.174574\n",
      "iteration 900 / 1500: loss 2.192630\n",
      "iteration 1000 / 1500: loss 2.163843\n",
      "iteration 1100 / 1500: loss 2.070829\n",
      "iteration 1200 / 1500: loss 2.130640\n",
      "iteration 1300 / 1500: loss 2.140960\n",
      "iteration 1400 / 1500: loss 2.166583\n",
      "iteration 0 / 1500: loss 1037.895163\n",
      "iteration 100 / 1500: loss 89.805706\n",
      "iteration 200 / 1500: loss 9.598727\n",
      "iteration 300 / 1500: loss 2.797837\n",
      "iteration 400 / 1500: loss 2.186329\n",
      "iteration 500 / 1500: loss 2.174319\n",
      "iteration 600 / 1500: loss 2.064958\n",
      "iteration 700 / 1500: loss 2.165007\n",
      "iteration 800 / 1500: loss 2.093862\n",
      "iteration 900 / 1500: loss 2.136810\n",
      "iteration 1000 / 1500: loss 2.113052\n",
      "iteration 1100 / 1500: loss 2.119934\n",
      "iteration 1200 / 1500: loss 2.173882\n",
      "iteration 1300 / 1500: loss 2.090716\n",
      "iteration 1400 / 1500: loss 2.213184\n",
      "iteration 0 / 1500: loss 1116.182875\n",
      "iteration 100 / 1500: loss 78.745277\n",
      "iteration 200 / 1500: loss 7.340399\n",
      "iteration 300 / 1500: loss 2.519536\n",
      "iteration 400 / 1500: loss 2.122230\n",
      "iteration 500 / 1500: loss 2.158360\n",
      "iteration 600 / 1500: loss 2.169587\n",
      "iteration 700 / 1500: loss 2.206099\n",
      "iteration 800 / 1500: loss 2.135031\n",
      "iteration 900 / 1500: loss 2.115973\n",
      "iteration 1000 / 1500: loss 2.119312\n",
      "iteration 1100 / 1500: loss 2.094651\n",
      "iteration 1200 / 1500: loss 2.153298\n",
      "iteration 1300 / 1500: loss 2.177563\n",
      "iteration 1400 / 1500: loss 2.187639\n",
      "iteration 0 / 1500: loss 1198.049716\n",
      "iteration 100 / 1500: loss 69.012056\n",
      "iteration 200 / 1500: loss 5.847484\n",
      "iteration 300 / 1500: loss 2.409042\n",
      "iteration 400 / 1500: loss 2.166218\n",
      "iteration 500 / 1500: loss 2.189385\n",
      "iteration 600 / 1500: loss 2.174350\n",
      "iteration 700 / 1500: loss 2.101642\n",
      "iteration 800 / 1500: loss 2.170433\n",
      "iteration 900 / 1500: loss 2.125865\n",
      "iteration 1000 / 1500: loss 2.154290\n",
      "iteration 1100 / 1500: loss 2.174658\n",
      "iteration 1200 / 1500: loss 2.059235\n",
      "iteration 1300 / 1500: loss 2.112641\n",
      "iteration 1400 / 1500: loss 2.179740\n",
      "iteration 0 / 1500: loss 1264.824671\n",
      "iteration 100 / 1500: loss 59.711871\n",
      "iteration 200 / 1500: loss 4.792947\n",
      "iteration 300 / 1500: loss 2.268958\n",
      "iteration 400 / 1500: loss 2.088210\n",
      "iteration 500 / 1500: loss 2.109753\n",
      "iteration 600 / 1500: loss 2.235700\n",
      "iteration 700 / 1500: loss 2.133529\n",
      "iteration 800 / 1500: loss 2.195282\n",
      "iteration 900 / 1500: loss 2.135412\n",
      "iteration 1000 / 1500: loss 2.121040\n",
      "iteration 1100 / 1500: loss 2.223712\n",
      "iteration 1200 / 1500: loss 2.198235\n",
      "iteration 1300 / 1500: loss 2.204737\n",
      "iteration 1400 / 1500: loss 2.199527\n",
      "iteration 0 / 1500: loss 1372.689513\n",
      "iteration 100 / 1500: loss 52.833150\n",
      "iteration 200 / 1500: loss 4.142610\n",
      "iteration 300 / 1500: loss 2.174948\n",
      "iteration 400 / 1500: loss 2.174939\n",
      "iteration 500 / 1500: loss 2.162986\n",
      "iteration 600 / 1500: loss 2.149466\n",
      "iteration 700 / 1500: loss 2.161574\n",
      "iteration 800 / 1500: loss 2.155945\n",
      "iteration 900 / 1500: loss 2.124750\n",
      "iteration 1000 / 1500: loss 2.190719\n",
      "iteration 1100 / 1500: loss 2.185619\n",
      "iteration 1200 / 1500: loss 2.195000\n",
      "iteration 1300 / 1500: loss 2.161609\n",
      "iteration 1400 / 1500: loss 2.112394\n",
      "iteration 0 / 1500: loss 1441.671740\n",
      "iteration 100 / 1500: loss 45.475412\n",
      "iteration 200 / 1500: loss 3.470459\n",
      "iteration 300 / 1500: loss 2.264586\n",
      "iteration 400 / 1500: loss 2.180144\n",
      "iteration 500 / 1500: loss 2.198108\n",
      "iteration 600 / 1500: loss 2.195633\n",
      "iteration 700 / 1500: loss 2.108457\n",
      "iteration 800 / 1500: loss 2.234839\n",
      "iteration 900 / 1500: loss 2.182973\n",
      "iteration 1000 / 1500: loss 2.235886\n",
      "iteration 1100 / 1500: loss 2.180246\n",
      "iteration 1200 / 1500: loss 2.170192\n",
      "iteration 1300 / 1500: loss 2.251237\n",
      "iteration 1400 / 1500: loss 2.168999\n",
      "iteration 0 / 1500: loss 1548.562164\n",
      "iteration 100 / 1500: loss 39.868879\n",
      "iteration 200 / 1500: loss 3.084739\n",
      "iteration 300 / 1500: loss 2.216615\n",
      "iteration 400 / 1500: loss 2.168831\n",
      "iteration 500 / 1500: loss 2.232847\n",
      "iteration 600 / 1500: loss 2.204942\n",
      "iteration 700 / 1500: loss 2.150190\n",
      "iteration 800 / 1500: loss 2.199419\n",
      "iteration 900 / 1500: loss 2.145143\n",
      "iteration 1000 / 1500: loss 2.123839\n",
      "iteration 1100 / 1500: loss 2.180055\n",
      "iteration 1200 / 1500: loss 2.146868\n",
      "iteration 1300 / 1500: loss 2.160554\n",
      "iteration 1400 / 1500: loss 2.198340\n",
      "iteration 0 / 1500: loss 780.681029\n",
      "iteration 100 / 1500: loss 99.421733\n",
      "iteration 200 / 1500: loss 14.359710\n",
      "iteration 300 / 1500: loss 3.571052\n",
      "iteration 400 / 1500: loss 2.324952\n",
      "iteration 500 / 1500: loss 2.051637\n",
      "iteration 600 / 1500: loss 2.116678\n",
      "iteration 700 / 1500: loss 2.220530\n",
      "iteration 800 / 1500: loss 2.165951\n",
      "iteration 900 / 1500: loss 2.131814\n",
      "iteration 1000 / 1500: loss 2.118394\n",
      "iteration 1100 / 1500: loss 2.121125\n",
      "iteration 1200 / 1500: loss 2.044824\n",
      "iteration 1300 / 1500: loss 2.196202\n",
      "iteration 1400 / 1500: loss 2.062774\n",
      "iteration 0 / 1500: loss 863.039430\n",
      "iteration 100 / 1500: loss 87.657752\n",
      "iteration 200 / 1500: loss 10.590252\n",
      "iteration 300 / 1500: loss 3.001485\n",
      "iteration 400 / 1500: loss 2.200443\n",
      "iteration 500 / 1500: loss 2.153048\n",
      "iteration 600 / 1500: loss 2.164548\n",
      "iteration 700 / 1500: loss 2.182074\n",
      "iteration 800 / 1500: loss 2.158053\n",
      "iteration 900 / 1500: loss 2.129151\n",
      "iteration 1000 / 1500: loss 2.185786\n",
      "iteration 1100 / 1500: loss 2.156993\n",
      "iteration 1200 / 1500: loss 2.116218\n",
      "iteration 1300 / 1500: loss 2.085831\n",
      "iteration 1400 / 1500: loss 2.080770\n",
      "iteration 0 / 1500: loss 949.124181\n",
      "iteration 100 / 1500: loss 76.819510\n",
      "iteration 200 / 1500: loss 7.984253\n",
      "iteration 300 / 1500: loss 2.638210\n",
      "iteration 400 / 1500: loss 2.155651\n",
      "iteration 500 / 1500: loss 2.167560\n",
      "iteration 600 / 1500: loss 2.136049\n",
      "iteration 700 / 1500: loss 2.140755\n",
      "iteration 800 / 1500: loss 2.110946\n",
      "iteration 900 / 1500: loss 2.156209\n",
      "iteration 1000 / 1500: loss 2.184953\n",
      "iteration 1100 / 1500: loss 2.121045\n",
      "iteration 1200 / 1500: loss 2.124412\n",
      "iteration 1300 / 1500: loss 2.226091\n",
      "iteration 1400 / 1500: loss 2.122500\n",
      "iteration 0 / 1500: loss 1023.312710\n",
      "iteration 100 / 1500: loss 66.213774\n",
      "iteration 200 / 1500: loss 6.176785\n",
      "iteration 300 / 1500: loss 2.430772\n",
      "iteration 400 / 1500: loss 2.113518\n",
      "iteration 500 / 1500: loss 2.153711\n",
      "iteration 600 / 1500: loss 2.121991\n",
      "iteration 700 / 1500: loss 2.136858\n",
      "iteration 800 / 1500: loss 2.109446\n",
      "iteration 900 / 1500: loss 2.136904\n",
      "iteration 1000 / 1500: loss 2.128738\n",
      "iteration 1100 / 1500: loss 2.113807\n",
      "iteration 1200 / 1500: loss 2.188164\n",
      "iteration 1300 / 1500: loss 2.187720\n",
      "iteration 1400 / 1500: loss 2.133749\n",
      "iteration 0 / 1500: loss 1091.649902\n",
      "iteration 100 / 1500: loss 56.237080\n",
      "iteration 200 / 1500: loss 4.776257\n",
      "iteration 300 / 1500: loss 2.344619\n",
      "iteration 400 / 1500: loss 2.091644\n",
      "iteration 500 / 1500: loss 2.081015\n",
      "iteration 600 / 1500: loss 2.187021\n",
      "iteration 700 / 1500: loss 2.205108\n",
      "iteration 800 / 1500: loss 2.147269\n",
      "iteration 900 / 1500: loss 2.119042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.155857\n",
      "iteration 1100 / 1500: loss 2.160067\n",
      "iteration 1200 / 1500: loss 2.130722\n",
      "iteration 1300 / 1500: loss 2.149805\n",
      "iteration 1400 / 1500: loss 2.166230\n",
      "iteration 0 / 1500: loss 1208.102377\n",
      "iteration 100 / 1500: loss 49.746938\n",
      "iteration 200 / 1500: loss 4.006985\n",
      "iteration 300 / 1500: loss 2.268195\n",
      "iteration 400 / 1500: loss 2.186660\n",
      "iteration 500 / 1500: loss 2.183196\n",
      "iteration 600 / 1500: loss 2.173306\n",
      "iteration 700 / 1500: loss 2.155447\n",
      "iteration 800 / 1500: loss 2.192339\n",
      "iteration 900 / 1500: loss 2.189728\n",
      "iteration 1000 / 1500: loss 2.152933\n",
      "iteration 1100 / 1500: loss 2.181426\n",
      "iteration 1200 / 1500: loss 2.168329\n",
      "iteration 1300 / 1500: loss 2.199555\n",
      "iteration 1400 / 1500: loss 2.130962\n",
      "iteration 0 / 1500: loss 1280.880328\n",
      "iteration 100 / 1500: loss 42.173918\n",
      "iteration 200 / 1500: loss 3.456391\n",
      "iteration 300 / 1500: loss 2.182451\n",
      "iteration 400 / 1500: loss 2.165574\n",
      "iteration 500 / 1500: loss 2.272505\n",
      "iteration 600 / 1500: loss 2.118447\n",
      "iteration 700 / 1500: loss 2.108825\n",
      "iteration 800 / 1500: loss 2.173375\n",
      "iteration 900 / 1500: loss 2.164626\n",
      "iteration 1000 / 1500: loss 2.161638\n",
      "iteration 1100 / 1500: loss 2.102314\n",
      "iteration 1200 / 1500: loss 2.196205\n",
      "iteration 1300 / 1500: loss 2.217038\n",
      "iteration 1400 / 1500: loss 2.207763\n",
      "iteration 0 / 1500: loss 1373.317235\n",
      "iteration 100 / 1500: loss 36.129104\n",
      "iteration 200 / 1500: loss 2.948613\n",
      "iteration 300 / 1500: loss 2.245882\n",
      "iteration 400 / 1500: loss 2.140558\n",
      "iteration 500 / 1500: loss 2.079280\n",
      "iteration 600 / 1500: loss 2.217258\n",
      "iteration 700 / 1500: loss 2.125335\n",
      "iteration 800 / 1500: loss 2.156393\n",
      "iteration 900 / 1500: loss 2.207845\n",
      "iteration 1000 / 1500: loss 2.133015\n",
      "iteration 1100 / 1500: loss 2.204473\n",
      "iteration 1200 / 1500: loss 2.129954\n",
      "iteration 1300 / 1500: loss 2.234850\n",
      "iteration 1400 / 1500: loss 2.144665\n",
      "iteration 0 / 1500: loss 1451.256327\n",
      "iteration 100 / 1500: loss 30.523690\n",
      "iteration 200 / 1500: loss 2.699905\n",
      "iteration 300 / 1500: loss 2.135317\n",
      "iteration 400 / 1500: loss 2.169922\n",
      "iteration 500 / 1500: loss 2.080551\n",
      "iteration 600 / 1500: loss 2.209706\n",
      "iteration 700 / 1500: loss 2.208080\n",
      "iteration 800 / 1500: loss 2.175495\n",
      "iteration 900 / 1500: loss 2.102456\n",
      "iteration 1000 / 1500: loss 2.158041\n",
      "iteration 1100 / 1500: loss 2.141025\n",
      "iteration 1200 / 1500: loss 2.217017\n",
      "iteration 1300 / 1500: loss 2.201253\n",
      "iteration 1400 / 1500: loss 2.171553\n",
      "iteration 0 / 1500: loss 1545.749064\n",
      "iteration 100 / 1500: loss 26.194918\n",
      "iteration 200 / 1500: loss 2.550136\n",
      "iteration 300 / 1500: loss 2.150038\n",
      "iteration 400 / 1500: loss 2.124626\n",
      "iteration 500 / 1500: loss 2.197919\n",
      "iteration 600 / 1500: loss 2.230600\n",
      "iteration 700 / 1500: loss 2.166872\n",
      "iteration 800 / 1500: loss 2.202895\n",
      "iteration 900 / 1500: loss 2.212091\n",
      "iteration 1000 / 1500: loss 2.147310\n",
      "iteration 1100 / 1500: loss 2.196184\n",
      "iteration 1200 / 1500: loss 2.155139\n",
      "iteration 1300 / 1500: loss 2.162977\n",
      "iteration 1400 / 1500: loss 2.142148\n",
      "iteration 0 / 1500: loss 764.270903\n",
      "iteration 100 / 1500: loss 78.254152\n",
      "iteration 200 / 1500: loss 9.771890\n",
      "iteration 300 / 1500: loss 2.956039\n",
      "iteration 400 / 1500: loss 2.183474\n",
      "iteration 500 / 1500: loss 2.095738\n",
      "iteration 600 / 1500: loss 2.131947\n",
      "iteration 700 / 1500: loss 2.145261\n",
      "iteration 800 / 1500: loss 2.093979\n",
      "iteration 900 / 1500: loss 2.117598\n",
      "iteration 1000 / 1500: loss 2.084393\n",
      "iteration 1100 / 1500: loss 2.118283\n",
      "iteration 1200 / 1500: loss 2.096055\n",
      "iteration 1300 / 1500: loss 2.090188\n",
      "iteration 1400 / 1500: loss 2.089828\n",
      "iteration 0 / 1500: loss 857.386773\n",
      "iteration 100 / 1500: loss 68.256023\n",
      "iteration 200 / 1500: loss 7.301178\n",
      "iteration 300 / 1500: loss 2.510343\n",
      "iteration 400 / 1500: loss 2.106735\n",
      "iteration 500 / 1500: loss 2.139313\n",
      "iteration 600 / 1500: loss 2.140604\n",
      "iteration 700 / 1500: loss 2.029351\n",
      "iteration 800 / 1500: loss 2.041085\n",
      "iteration 900 / 1500: loss 2.143662\n",
      "iteration 1000 / 1500: loss 2.084492\n",
      "iteration 1100 / 1500: loss 2.061525\n",
      "iteration 1200 / 1500: loss 2.098420\n",
      "iteration 1300 / 1500: loss 2.094551\n",
      "iteration 1400 / 1500: loss 2.151504\n",
      "iteration 0 / 1500: loss 941.513058\n",
      "iteration 100 / 1500: loss 58.340790\n",
      "iteration 200 / 1500: loss 5.559265\n",
      "iteration 300 / 1500: loss 2.349254\n",
      "iteration 400 / 1500: loss 2.141782\n",
      "iteration 500 / 1500: loss 2.153050\n",
      "iteration 600 / 1500: loss 2.159798\n",
      "iteration 700 / 1500: loss 2.153231\n",
      "iteration 800 / 1500: loss 2.138952\n",
      "iteration 900 / 1500: loss 2.125443\n",
      "iteration 1000 / 1500: loss 2.165680\n",
      "iteration 1100 / 1500: loss 2.190867\n",
      "iteration 1200 / 1500: loss 2.182185\n",
      "iteration 1300 / 1500: loss 2.114018\n",
      "iteration 1400 / 1500: loss 2.171232\n",
      "iteration 0 / 1500: loss 1026.783353\n",
      "iteration 100 / 1500: loss 49.577170\n",
      "iteration 200 / 1500: loss 4.343322\n",
      "iteration 300 / 1500: loss 2.240862\n",
      "iteration 400 / 1500: loss 2.133464\n",
      "iteration 500 / 1500: loss 2.158519\n",
      "iteration 600 / 1500: loss 2.126767\n",
      "iteration 700 / 1500: loss 2.186947\n",
      "iteration 800 / 1500: loss 2.153615\n",
      "iteration 900 / 1500: loss 2.119884\n",
      "iteration 1000 / 1500: loss 2.126397\n",
      "iteration 1100 / 1500: loss 2.215366\n",
      "iteration 1200 / 1500: loss 2.076150\n",
      "iteration 1300 / 1500: loss 2.231606\n",
      "iteration 1400 / 1500: loss 2.068613\n",
      "iteration 0 / 1500: loss 1107.327605\n",
      "iteration 100 / 1500: loss 41.697020\n",
      "iteration 200 / 1500: loss 3.709266\n",
      "iteration 300 / 1500: loss 2.230303\n",
      "iteration 400 / 1500: loss 2.147779\n",
      "iteration 500 / 1500: loss 2.100527\n",
      "iteration 600 / 1500: loss 2.156787\n",
      "iteration 700 / 1500: loss 2.166381\n",
      "iteration 800 / 1500: loss 2.149151\n",
      "iteration 900 / 1500: loss 2.177796\n",
      "iteration 1000 / 1500: loss 2.212784\n",
      "iteration 1100 / 1500: loss 2.078591\n",
      "iteration 1200 / 1500: loss 2.142959\n",
      "iteration 1300 / 1500: loss 2.160174\n",
      "iteration 1400 / 1500: loss 2.165703\n",
      "iteration 0 / 1500: loss 1200.430635\n",
      "iteration 100 / 1500: loss 35.335529\n",
      "iteration 200 / 1500: loss 3.022838\n",
      "iteration 300 / 1500: loss 2.181127\n",
      "iteration 400 / 1500: loss 2.109825\n",
      "iteration 500 / 1500: loss 2.154616\n",
      "iteration 600 / 1500: loss 2.097660\n",
      "iteration 700 / 1500: loss 2.129118\n",
      "iteration 800 / 1500: loss 2.148555\n",
      "iteration 900 / 1500: loss 2.151496\n",
      "iteration 1000 / 1500: loss 2.115434\n",
      "iteration 1100 / 1500: loss 2.161760\n",
      "iteration 1200 / 1500: loss 2.132304\n",
      "iteration 1300 / 1500: loss 2.119020\n",
      "iteration 1400 / 1500: loss 2.055782\n",
      "iteration 0 / 1500: loss 1263.898381\n",
      "iteration 100 / 1500: loss 29.110540\n",
      "iteration 200 / 1500: loss 2.756583\n",
      "iteration 300 / 1500: loss 2.137975\n",
      "iteration 400 / 1500: loss 2.207009\n",
      "iteration 500 / 1500: loss 2.126913\n",
      "iteration 600 / 1500: loss 2.135179\n",
      "iteration 700 / 1500: loss 2.162721\n",
      "iteration 800 / 1500: loss 2.168208\n",
      "iteration 900 / 1500: loss 2.176352\n",
      "iteration 1000 / 1500: loss 2.152650\n",
      "iteration 1100 / 1500: loss 2.128260\n",
      "iteration 1200 / 1500: loss 2.186883\n",
      "iteration 1300 / 1500: loss 2.109754\n",
      "iteration 1400 / 1500: loss 2.177327\n",
      "iteration 0 / 1500: loss 1362.543989\n",
      "iteration 100 / 1500: loss 24.598678\n",
      "iteration 200 / 1500: loss 2.537783\n",
      "iteration 300 / 1500: loss 2.168507\n",
      "iteration 400 / 1500: loss 2.179148\n",
      "iteration 500 / 1500: loss 2.081025\n",
      "iteration 600 / 1500: loss 2.242062\n",
      "iteration 700 / 1500: loss 2.089914\n",
      "iteration 800 / 1500: loss 2.156935\n",
      "iteration 900 / 1500: loss 2.234442\n",
      "iteration 1000 / 1500: loss 2.161224\n",
      "iteration 1100 / 1500: loss 2.225267\n",
      "iteration 1200 / 1500: loss 2.140309\n",
      "iteration 1300 / 1500: loss 2.158932\n",
      "iteration 1400 / 1500: loss 2.136987\n",
      "iteration 0 / 1500: loss 1452.504685\n",
      "iteration 100 / 1500: loss 20.634404\n",
      "iteration 200 / 1500: loss 2.464956\n",
      "iteration 300 / 1500: loss 2.164248\n",
      "iteration 400 / 1500: loss 2.185706\n",
      "iteration 500 / 1500: loss 2.190396\n",
      "iteration 600 / 1500: loss 2.212152\n",
      "iteration 700 / 1500: loss 2.224899\n",
      "iteration 800 / 1500: loss 2.105282\n",
      "iteration 900 / 1500: loss 2.102846\n",
      "iteration 1000 / 1500: loss 2.175095\n",
      "iteration 1100 / 1500: loss 2.097936\n",
      "iteration 1200 / 1500: loss 2.200615\n",
      "iteration 1300 / 1500: loss 2.185194\n",
      "iteration 1400 / 1500: loss 2.216966\n",
      "iteration 0 / 1500: loss 1549.738145\n",
      "iteration 100 / 1500: loss 17.407365\n",
      "iteration 200 / 1500: loss 2.392657\n",
      "iteration 300 / 1500: loss 2.201128\n",
      "iteration 400 / 1500: loss 2.130509\n",
      "iteration 500 / 1500: loss 2.165943\n",
      "iteration 600 / 1500: loss 2.226245\n",
      "iteration 700 / 1500: loss 2.148964\n",
      "iteration 800 / 1500: loss 2.239317\n",
      "iteration 900 / 1500: loss 2.140210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.192373\n",
      "iteration 1100 / 1500: loss 2.166961\n",
      "iteration 1200 / 1500: loss 2.106492\n",
      "iteration 1300 / 1500: loss 2.187994\n",
      "iteration 1400 / 1500: loss 2.160915\n",
      "iteration 0 / 1500: loss 775.387906\n",
      "iteration 100 / 1500: loss 63.773285\n",
      "iteration 200 / 1500: loss 7.079299\n",
      "iteration 300 / 1500: loss 2.508487\n",
      "iteration 400 / 1500: loss 2.119511\n",
      "iteration 500 / 1500: loss 2.164326\n",
      "iteration 600 / 1500: loss 2.070277\n",
      "iteration 700 / 1500: loss 2.064794\n",
      "iteration 800 / 1500: loss 2.072973\n",
      "iteration 900 / 1500: loss 2.100921\n",
      "iteration 1000 / 1500: loss 2.122651\n",
      "iteration 1100 / 1500: loss 2.131531\n",
      "iteration 1200 / 1500: loss 2.132283\n",
      "iteration 1300 / 1500: loss 2.171164\n",
      "iteration 1400 / 1500: loss 2.095797\n",
      "iteration 0 / 1500: loss 871.884680\n",
      "iteration 100 / 1500: loss 54.423073\n",
      "iteration 200 / 1500: loss 5.294054\n",
      "iteration 300 / 1500: loss 2.320777\n",
      "iteration 400 / 1500: loss 2.111642\n",
      "iteration 500 / 1500: loss 2.116239\n",
      "iteration 600 / 1500: loss 2.111457\n",
      "iteration 700 / 1500: loss 2.170963\n",
      "iteration 800 / 1500: loss 2.192695\n",
      "iteration 900 / 1500: loss 2.066194\n",
      "iteration 1000 / 1500: loss 2.094623\n",
      "iteration 1100 / 1500: loss 2.107921\n",
      "iteration 1200 / 1500: loss 2.165653\n",
      "iteration 1300 / 1500: loss 2.060784\n",
      "iteration 1400 / 1500: loss 2.099036\n",
      "iteration 0 / 1500: loss 946.502778\n",
      "iteration 100 / 1500: loss 44.901823\n",
      "iteration 200 / 1500: loss 4.113932\n",
      "iteration 300 / 1500: loss 2.264188\n",
      "iteration 400 / 1500: loss 2.091164\n",
      "iteration 500 / 1500: loss 2.238329\n",
      "iteration 600 / 1500: loss 2.116484\n",
      "iteration 700 / 1500: loss 2.163318\n",
      "iteration 800 / 1500: loss 2.147792\n",
      "iteration 900 / 1500: loss 2.103407\n",
      "iteration 1000 / 1500: loss 2.215755\n",
      "iteration 1100 / 1500: loss 2.112722\n",
      "iteration 1200 / 1500: loss 2.187061\n",
      "iteration 1300 / 1500: loss 2.242991\n",
      "iteration 1400 / 1500: loss 2.132267\n",
      "iteration 0 / 1500: loss 1036.174096\n",
      "iteration 100 / 1500: loss 37.563021\n",
      "iteration 200 / 1500: loss 3.312391\n",
      "iteration 300 / 1500: loss 2.181996\n",
      "iteration 400 / 1500: loss 2.128437\n",
      "iteration 500 / 1500: loss 2.112444\n",
      "iteration 600 / 1500: loss 2.117671\n",
      "iteration 700 / 1500: loss 2.132353\n",
      "iteration 800 / 1500: loss 2.118444\n",
      "iteration 900 / 1500: loss 2.155880\n",
      "iteration 1000 / 1500: loss 2.130405\n",
      "iteration 1100 / 1500: loss 2.150740\n",
      "iteration 1200 / 1500: loss 2.112135\n",
      "iteration 1300 / 1500: loss 2.150669\n",
      "iteration 1400 / 1500: loss 2.157046\n",
      "iteration 0 / 1500: loss 1112.690102\n",
      "iteration 100 / 1500: loss 30.838865\n",
      "iteration 200 / 1500: loss 2.858176\n",
      "iteration 300 / 1500: loss 2.253280\n",
      "iteration 400 / 1500: loss 2.212894\n",
      "iteration 500 / 1500: loss 2.189032\n",
      "iteration 600 / 1500: loss 2.178460\n",
      "iteration 700 / 1500: loss 2.149126\n",
      "iteration 800 / 1500: loss 2.111437\n",
      "iteration 900 / 1500: loss 2.185167\n",
      "iteration 1000 / 1500: loss 2.164779\n",
      "iteration 1100 / 1500: loss 2.180827\n",
      "iteration 1200 / 1500: loss 2.074594\n",
      "iteration 1300 / 1500: loss 2.192236\n",
      "iteration 1400 / 1500: loss 2.166958\n",
      "iteration 0 / 1500: loss 1213.052228\n",
      "iteration 100 / 1500: loss 25.643995\n",
      "iteration 200 / 1500: loss 2.549098\n",
      "iteration 300 / 1500: loss 2.220745\n",
      "iteration 400 / 1500: loss 2.113957\n",
      "iteration 500 / 1500: loss 2.135774\n",
      "iteration 600 / 1500: loss 2.115057\n",
      "iteration 700 / 1500: loss 2.062998\n",
      "iteration 800 / 1500: loss 2.119165\n",
      "iteration 900 / 1500: loss 2.119648\n",
      "iteration 1000 / 1500: loss 2.168484\n",
      "iteration 1100 / 1500: loss 2.151379\n",
      "iteration 1200 / 1500: loss 2.184984\n",
      "iteration 1300 / 1500: loss 2.158565\n",
      "iteration 1400 / 1500: loss 2.164504\n",
      "iteration 0 / 1500: loss 1303.083653\n",
      "iteration 100 / 1500: loss 21.255280\n",
      "iteration 200 / 1500: loss 2.464445\n",
      "iteration 300 / 1500: loss 2.137494\n",
      "iteration 400 / 1500: loss 2.145387\n",
      "iteration 500 / 1500: loss 2.208220\n",
      "iteration 600 / 1500: loss 2.160174\n",
      "iteration 700 / 1500: loss 2.204129\n",
      "iteration 800 / 1500: loss 2.161907\n",
      "iteration 900 / 1500: loss 2.238164\n",
      "iteration 1000 / 1500: loss 2.181024\n",
      "iteration 1100 / 1500: loss 2.181982\n",
      "iteration 1200 / 1500: loss 2.192949\n",
      "iteration 1300 / 1500: loss 2.183191\n",
      "iteration 1400 / 1500: loss 2.178204\n",
      "iteration 0 / 1500: loss 1394.622429\n",
      "iteration 100 / 1500: loss 17.560727\n",
      "iteration 200 / 1500: loss 2.312129\n",
      "iteration 300 / 1500: loss 2.132232\n",
      "iteration 400 / 1500: loss 2.197899\n",
      "iteration 500 / 1500: loss 2.162227\n",
      "iteration 600 / 1500: loss 2.173851\n",
      "iteration 700 / 1500: loss 2.244159\n",
      "iteration 800 / 1500: loss 2.159339\n",
      "iteration 900 / 1500: loss 2.150849\n",
      "iteration 1000 / 1500: loss 2.172760\n",
      "iteration 1100 / 1500: loss 2.197192\n",
      "iteration 1200 / 1500: loss 2.191523\n",
      "iteration 1300 / 1500: loss 2.150934\n",
      "iteration 1400 / 1500: loss 2.173934\n",
      "iteration 0 / 1500: loss 1447.788013\n",
      "iteration 100 / 1500: loss 14.112449\n",
      "iteration 200 / 1500: loss 2.349519\n",
      "iteration 300 / 1500: loss 2.234773\n",
      "iteration 400 / 1500: loss 2.182944\n",
      "iteration 500 / 1500: loss 2.248564\n",
      "iteration 600 / 1500: loss 2.157928\n",
      "iteration 700 / 1500: loss 2.233596\n",
      "iteration 800 / 1500: loss 2.156373\n",
      "iteration 900 / 1500: loss 2.179082\n",
      "iteration 1000 / 1500: loss 2.146299\n",
      "iteration 1100 / 1500: loss 2.117543\n",
      "iteration 1200 / 1500: loss 2.164201\n",
      "iteration 1300 / 1500: loss 2.129427\n",
      "iteration 1400 / 1500: loss 2.152037\n",
      "iteration 0 / 1500: loss 1542.808464\n",
      "iteration 100 / 1500: loss 11.803388\n",
      "iteration 200 / 1500: loss 2.203241\n",
      "iteration 300 / 1500: loss 2.206975\n",
      "iteration 400 / 1500: loss 2.178451\n",
      "iteration 500 / 1500: loss 2.210800\n",
      "iteration 600 / 1500: loss 2.143796\n",
      "iteration 700 / 1500: loss 2.160392\n",
      "iteration 800 / 1500: loss 2.168648\n",
      "iteration 900 / 1500: loss 2.094562\n",
      "iteration 1000 / 1500: loss 2.221813\n",
      "iteration 1100 / 1500: loss 2.224247\n",
      "iteration 1200 / 1500: loss 2.153935\n",
      "iteration 1300 / 1500: loss 2.133636\n",
      "iteration 1400 / 1500: loss 2.180602\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.351796 val accuracy: 0.368000\n",
      "lr 1.000000e-07 reg 2.777778e+04 train accuracy: 0.350878 val accuracy: 0.366000\n",
      "lr 1.000000e-07 reg 3.055556e+04 train accuracy: 0.346184 val accuracy: 0.368000\n",
      "lr 1.000000e-07 reg 3.333333e+04 train accuracy: 0.342143 val accuracy: 0.355000\n",
      "lr 1.000000e-07 reg 3.611111e+04 train accuracy: 0.338143 val accuracy: 0.354000\n",
      "lr 1.000000e-07 reg 3.888889e+04 train accuracy: 0.338224 val accuracy: 0.352000\n",
      "lr 1.000000e-07 reg 4.166667e+04 train accuracy: 0.333939 val accuracy: 0.354000\n",
      "lr 1.000000e-07 reg 4.444444e+04 train accuracy: 0.332510 val accuracy: 0.344000\n",
      "lr 1.000000e-07 reg 4.722222e+04 train accuracy: 0.335327 val accuracy: 0.346000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.326837 val accuracy: 0.340000\n",
      "lr 1.444444e-07 reg 2.500000e+04 train accuracy: 0.352327 val accuracy: 0.365000\n",
      "lr 1.444444e-07 reg 2.777778e+04 train accuracy: 0.349245 val accuracy: 0.360000\n",
      "lr 1.444444e-07 reg 3.055556e+04 train accuracy: 0.341898 val accuracy: 0.360000\n",
      "lr 1.444444e-07 reg 3.333333e+04 train accuracy: 0.342653 val accuracy: 0.357000\n",
      "lr 1.444444e-07 reg 3.611111e+04 train accuracy: 0.341980 val accuracy: 0.361000\n",
      "lr 1.444444e-07 reg 3.888889e+04 train accuracy: 0.335653 val accuracy: 0.348000\n",
      "lr 1.444444e-07 reg 4.166667e+04 train accuracy: 0.338653 val accuracy: 0.353000\n",
      "lr 1.444444e-07 reg 4.444444e+04 train accuracy: 0.330633 val accuracy: 0.351000\n",
      "lr 1.444444e-07 reg 4.722222e+04 train accuracy: 0.329347 val accuracy: 0.343000\n",
      "lr 1.444444e-07 reg 5.000000e+04 train accuracy: 0.327571 val accuracy: 0.341000\n",
      "lr 1.888889e-07 reg 2.500000e+04 train accuracy: 0.346796 val accuracy: 0.356000\n",
      "lr 1.888889e-07 reg 2.777778e+04 train accuracy: 0.347796 val accuracy: 0.359000\n",
      "lr 1.888889e-07 reg 3.055556e+04 train accuracy: 0.342224 val accuracy: 0.357000\n",
      "lr 1.888889e-07 reg 3.333333e+04 train accuracy: 0.338082 val accuracy: 0.352000\n",
      "lr 1.888889e-07 reg 3.611111e+04 train accuracy: 0.337714 val accuracy: 0.348000\n",
      "lr 1.888889e-07 reg 3.888889e+04 train accuracy: 0.337959 val accuracy: 0.352000\n",
      "lr 1.888889e-07 reg 4.166667e+04 train accuracy: 0.332857 val accuracy: 0.355000\n",
      "lr 1.888889e-07 reg 4.444444e+04 train accuracy: 0.335857 val accuracy: 0.339000\n",
      "lr 1.888889e-07 reg 4.722222e+04 train accuracy: 0.333286 val accuracy: 0.342000\n",
      "lr 1.888889e-07 reg 5.000000e+04 train accuracy: 0.328939 val accuracy: 0.346000\n",
      "lr 2.333333e-07 reg 2.500000e+04 train accuracy: 0.352449 val accuracy: 0.365000\n",
      "lr 2.333333e-07 reg 2.777778e+04 train accuracy: 0.351429 val accuracy: 0.361000\n",
      "lr 2.333333e-07 reg 3.055556e+04 train accuracy: 0.344429 val accuracy: 0.354000\n",
      "lr 2.333333e-07 reg 3.333333e+04 train accuracy: 0.336327 val accuracy: 0.353000\n",
      "lr 2.333333e-07 reg 3.611111e+04 train accuracy: 0.339306 val accuracy: 0.345000\n",
      "lr 2.333333e-07 reg 3.888889e+04 train accuracy: 0.328633 val accuracy: 0.340000\n",
      "lr 2.333333e-07 reg 4.166667e+04 train accuracy: 0.335184 val accuracy: 0.356000\n",
      "lr 2.333333e-07 reg 4.444444e+04 train accuracy: 0.333265 val accuracy: 0.347000\n",
      "lr 2.333333e-07 reg 4.722222e+04 train accuracy: 0.328020 val accuracy: 0.351000\n",
      "lr 2.333333e-07 reg 5.000000e+04 train accuracy: 0.328184 val accuracy: 0.348000\n",
      "lr 2.777778e-07 reg 2.500000e+04 train accuracy: 0.350245 val accuracy: 0.356000\n",
      "lr 2.777778e-07 reg 2.777778e+04 train accuracy: 0.348367 val accuracy: 0.360000\n",
      "lr 2.777778e-07 reg 3.055556e+04 train accuracy: 0.346796 val accuracy: 0.358000\n",
      "lr 2.777778e-07 reg 3.333333e+04 train accuracy: 0.338265 val accuracy: 0.357000\n",
      "lr 2.777778e-07 reg 3.611111e+04 train accuracy: 0.335082 val accuracy: 0.354000\n",
      "lr 2.777778e-07 reg 3.888889e+04 train accuracy: 0.339082 val accuracy: 0.362000\n",
      "lr 2.777778e-07 reg 4.166667e+04 train accuracy: 0.330959 val accuracy: 0.340000\n",
      "lr 2.777778e-07 reg 4.444444e+04 train accuracy: 0.332714 val accuracy: 0.350000\n",
      "lr 2.777778e-07 reg 4.722222e+04 train accuracy: 0.325327 val accuracy: 0.342000\n",
      "lr 2.777778e-07 reg 5.000000e+04 train accuracy: 0.328388 val accuracy: 0.351000\n",
      "lr 3.222222e-07 reg 2.500000e+04 train accuracy: 0.351306 val accuracy: 0.372000\n",
      "lr 3.222222e-07 reg 2.777778e+04 train accuracy: 0.346898 val accuracy: 0.360000\n",
      "lr 3.222222e-07 reg 3.055556e+04 train accuracy: 0.345653 val accuracy: 0.359000\n",
      "lr 3.222222e-07 reg 3.333333e+04 train accuracy: 0.343041 val accuracy: 0.352000\n",
      "lr 3.222222e-07 reg 3.611111e+04 train accuracy: 0.341755 val accuracy: 0.351000\n",
      "lr 3.222222e-07 reg 3.888889e+04 train accuracy: 0.338612 val accuracy: 0.352000\n",
      "lr 3.222222e-07 reg 4.166667e+04 train accuracy: 0.328673 val accuracy: 0.349000\n",
      "lr 3.222222e-07 reg 4.444444e+04 train accuracy: 0.328531 val accuracy: 0.346000\n",
      "lr 3.222222e-07 reg 4.722222e+04 train accuracy: 0.330510 val accuracy: 0.345000\n",
      "lr 3.222222e-07 reg 5.000000e+04 train accuracy: 0.321551 val accuracy: 0.328000\n",
      "lr 3.666667e-07 reg 2.500000e+04 train accuracy: 0.350327 val accuracy: 0.361000\n",
      "lr 3.666667e-07 reg 2.777778e+04 train accuracy: 0.344980 val accuracy: 0.364000\n",
      "lr 3.666667e-07 reg 3.055556e+04 train accuracy: 0.343490 val accuracy: 0.355000\n",
      "lr 3.666667e-07 reg 3.333333e+04 train accuracy: 0.343367 val accuracy: 0.352000\n",
      "lr 3.666667e-07 reg 3.611111e+04 train accuracy: 0.339163 val accuracy: 0.356000\n",
      "lr 3.666667e-07 reg 3.888889e+04 train accuracy: 0.339224 val accuracy: 0.352000\n",
      "lr 3.666667e-07 reg 4.166667e+04 train accuracy: 0.330082 val accuracy: 0.340000\n",
      "lr 3.666667e-07 reg 4.444444e+04 train accuracy: 0.332878 val accuracy: 0.342000\n",
      "lr 3.666667e-07 reg 4.722222e+04 train accuracy: 0.327061 val accuracy: 0.344000\n",
      "lr 3.666667e-07 reg 5.000000e+04 train accuracy: 0.329143 val accuracy: 0.346000\n",
      "lr 4.111111e-07 reg 2.500000e+04 train accuracy: 0.343776 val accuracy: 0.349000\n",
      "lr 4.111111e-07 reg 2.777778e+04 train accuracy: 0.344694 val accuracy: 0.354000\n",
      "lr 4.111111e-07 reg 3.055556e+04 train accuracy: 0.338286 val accuracy: 0.345000\n",
      "lr 4.111111e-07 reg 3.333333e+04 train accuracy: 0.333735 val accuracy: 0.339000\n",
      "lr 4.111111e-07 reg 3.611111e+04 train accuracy: 0.334143 val accuracy: 0.346000\n",
      "lr 4.111111e-07 reg 3.888889e+04 train accuracy: 0.332224 val accuracy: 0.338000\n",
      "lr 4.111111e-07 reg 4.166667e+04 train accuracy: 0.329776 val accuracy: 0.348000\n",
      "lr 4.111111e-07 reg 4.444444e+04 train accuracy: 0.332980 val accuracy: 0.335000\n",
      "lr 4.111111e-07 reg 4.722222e+04 train accuracy: 0.335857 val accuracy: 0.336000\n",
      "lr 4.111111e-07 reg 5.000000e+04 train accuracy: 0.330000 val accuracy: 0.340000\n",
      "lr 4.555556e-07 reg 2.500000e+04 train accuracy: 0.350102 val accuracy: 0.363000\n",
      "lr 4.555556e-07 reg 2.777778e+04 train accuracy: 0.345633 val accuracy: 0.361000\n",
      "lr 4.555556e-07 reg 3.055556e+04 train accuracy: 0.339653 val accuracy: 0.351000\n",
      "lr 4.555556e-07 reg 3.333333e+04 train accuracy: 0.343082 val accuracy: 0.350000\n",
      "lr 4.555556e-07 reg 3.611111e+04 train accuracy: 0.340265 val accuracy: 0.339000\n",
      "lr 4.555556e-07 reg 3.888889e+04 train accuracy: 0.335388 val accuracy: 0.346000\n",
      "lr 4.555556e-07 reg 4.166667e+04 train accuracy: 0.333673 val accuracy: 0.348000\n",
      "lr 4.555556e-07 reg 4.444444e+04 train accuracy: 0.324327 val accuracy: 0.339000\n",
      "lr 4.555556e-07 reg 4.722222e+04 train accuracy: 0.318898 val accuracy: 0.335000\n",
      "lr 4.555556e-07 reg 5.000000e+04 train accuracy: 0.326531 val accuracy: 0.333000\n",
      "lr 5.000000e-07 reg 2.500000e+04 train accuracy: 0.341367 val accuracy: 0.357000\n",
      "lr 5.000000e-07 reg 2.777778e+04 train accuracy: 0.348694 val accuracy: 0.357000\n",
      "lr 5.000000e-07 reg 3.055556e+04 train accuracy: 0.330327 val accuracy: 0.352000\n",
      "lr 5.000000e-07 reg 3.333333e+04 train accuracy: 0.339653 val accuracy: 0.346000\n",
      "lr 5.000000e-07 reg 3.611111e+04 train accuracy: 0.333735 val accuracy: 0.350000\n",
      "lr 5.000000e-07 reg 3.888889e+04 train accuracy: 0.332776 val accuracy: 0.343000\n",
      "lr 5.000000e-07 reg 4.166667e+04 train accuracy: 0.330796 val accuracy: 0.342000\n",
      "lr 5.000000e-07 reg 4.444444e+04 train accuracy: 0.328490 val accuracy: 0.355000\n",
      "lr 5.000000e-07 reg 4.722222e+04 train accuracy: 0.328245 val accuracy: 0.341000\n",
      "lr 5.000000e-07 reg 5.000000e+04 train accuracy: 0.328429 val accuracy: 0.341000\n",
      "best validation accuracy achieved during cross-validation: 0.372000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from deep_learning_su.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for learning_rate in np.linspace(learning_rates[0], learning_rates[1], num=10):\n",
    "    for reg in np.linspace(regularization_strengths[0], regularization_strengths[1], num=10):\n",
    "        softmax = Softmax()\n",
    "        loss_hist = softmax.train(X_train, y_train, learning_rate, reg, num_iters=1500,\n",
    "                                  verbose=True)\n",
    "        \n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[learning_rate, reg] = training_accuracy, validation_accuracy\n",
    "        \n",
    "        if(validation_accuracy > best_val):\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.367000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('.env': virtualenv)",
   "language": "python",
   "name": "python37564bitenvvirtualenv660a35bb6a9d42b2bbb4db00b8488467"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
